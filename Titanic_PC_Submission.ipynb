{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "#load data \n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11 features, we can try to know how many different values have each of the features..(binary, categorical, conitnuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirem correlacions en plan bestia, viam que hi veiem a les dades brutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hombreeeee!, ja es veuen un parell de coses interessants: **Survived esta directament correlacionat amb Fare (mes fare, mes survived) i inversament amb Pclass (pclas 1, mes survival)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do some basic pivot tables and see what we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Destriem Deck i numero de la cabina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Ara podem aplicar el slice al df_train original i reomplir els NaNs del Deck amb X\n",
    "df_train[\"Deck\"] = df_train[\"Cabin\"].str.slice(0,1)\n",
    "df_train[\"Number\"] = df_train[\"Cabin\"].str.slice(1,4).str.extract('([0-9]+)').astype('float')\n",
    "df_train['Deck'][df_train['Deck'].isnull()]='X'\n",
    "\n",
    "df_test[\"Deck\"] = df_test[\"Cabin\"].str.slice(0,1)\n",
    "df_test[\"Number\"] = df_test[\"Cabin\"].str.slice(1,4).str.extract('([0-9]+)').astype('float')\n",
    "df_test['Deck'][df_test['Deck'].isnull()]='X'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provem un hot-encoding per obtenir una taula amb 1 i 0s per cada columna de Desk\n",
    "Haurem de fer una altra per si es home-dona, una altra per P-Class i edat? (abans hem de categoritzar-la)\n",
    "info de: https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install category_encoders\n",
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcio hot encoding. Es tracta d'una funcio que agafa una variable categorica (passada a array) i crea un array on cada categoria es una columna amb 0 o 1\n",
    "def hot_encode_PC(value_array):\n",
    "    \"\"\"Utility function convert variable to columns with 0 and 1, after you have to pass it manually de column names\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    print(integer_encoded)\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Deck T es un error, no existeix, nomes hi ha un passatger i dificulat analisi posterior perque al test no n'hi han. Per "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X' 'C' 'X' 'C' 'X' 'X' 'E' 'X' 'X' 'X' 'G' 'C' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'D' 'X' 'A' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'D' 'X'\n",
      " 'B' 'C' 'X' 'X' 'X' 'X' 'X' 'B' 'C' 'X' 'X' 'X' 'F' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'F' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'X'\n",
      " 'X' 'X' 'E' 'X' 'X' 'X' 'A' 'D' 'X' 'X' 'X' 'X' 'D' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'X' 'E' 'D' 'X'\n",
      " 'X' 'X' 'F' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'D' 'C' 'X' 'B' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'F' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'E' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'A' 'X' 'X' 'C' 'X' 'X'\n",
      " 'X' 'X' 'X' 'F' 'X' 'A' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'F' 'B' 'B' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'G' 'X' 'X' 'X' 'A' 'X' 'X' 'X' 'X' 'X' 'D'\n",
      " 'X' 'X' 'D' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'X' 'D' 'X' 'X' 'G'\n",
      " 'C' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'X' 'E' 'B' 'X' 'X' 'X' 'X' 'C' 'C'\n",
      " 'X' 'X' 'X' 'C' 'X' 'D' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'A' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'B' 'D' 'X' 'X' 'X' 'X' 'C' 'C' 'B' 'X' 'X' 'X' 'E' 'X' 'C'\n",
      " 'X' 'C' 'X' 'E' 'C' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'E' 'X' 'X' 'X' 'X'\n",
      " 'X' 'C' 'X' 'D' 'X' 'B' 'X' 'C' 'C' 'X' 'X' 'X' 'C' 'E' 'X' 'T' 'F' 'C'\n",
      " 'X' 'X' 'X' 'F' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'E' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'D' 'X' 'X' 'B' 'E' 'X' 'X' 'X' 'X' 'X' 'X' 'C'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'D' 'G' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'E' 'C' 'X'\n",
      " 'X' 'X' 'E' 'B' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'A' 'X' 'X' 'X' 'C'\n",
      " 'X' 'X' 'C' 'C' 'X' 'X' 'E' 'D' 'X' 'X' 'E' 'X' 'E' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'D' 'X' 'A' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X'\n",
      " 'C' 'B' 'X' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'D' 'X' 'C' 'X' 'X' 'X' 'X' 'X'\n",
      " 'B' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'E' 'X' 'X' 'D' 'F' 'X' 'X' 'X' 'B' 'X'\n",
      " 'X' 'B' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'B'\n",
      " 'B' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'A' 'X'\n",
      " 'E' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'E' 'X' 'X' 'X'\n",
      " 'X' 'E' 'X' 'X' 'X' 'C' 'X' 'A' 'X' 'E' 'X' 'B' 'X' 'X' 'X' 'D' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'A' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'F' 'X' 'X' 'D' 'X' 'X' 'X' 'D' 'X' 'D' 'X' 'X'\n",
      " 'A' 'X' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'D' 'X' 'A'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'D' 'X' 'X' 'E' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'C' 'X' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'D' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'B' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'F' 'C' 'E'\n",
      " 'X' 'X' 'X' 'X' 'X' 'E' 'X' 'X' 'C' 'C' 'C' 'X' 'X' 'F' 'C' 'E' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'E' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'B'\n",
      " 'X' 'X' 'D' 'C' 'B' 'X' 'X' 'B' 'X' 'X' 'D' 'X' 'X' 'E' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'B' 'X' 'D' 'X' 'X' 'X' 'X' 'X' 'X' 'E' 'X'\n",
      " 'X' 'X' 'F' 'X' 'X' 'B' 'X' 'B' 'D' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'D' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'A' 'X' 'X' 'E'\n",
      " 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'E' 'X' 'X' 'X' 'X'\n",
      " 'X' 'B' 'X' 'X' 'X' 'X' 'X' 'E' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'D' 'X' 'X' 'X' 'E' 'X' 'X' 'X' 'X' 'D' 'X'\n",
      " 'X' 'X' 'X' 'A' 'X' 'X' 'X' 'D' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'C' 'X']\n",
      "[8 2 8 2 8 8 4 8 8 8 6 2 8 8 8 8 8 8 8 8 8 3 8 0 8 8 8 2 8 8 8 1 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 3 8 1 2 8 8 8 8 8 1 2 8 8 8 5 8 8 8 8 8 8 8\n",
      " 8 5 8 8 8 8 8 8 8 8 8 8 8 8 2 8 8 8 4 8 8 8 0 3 8 8 8 8 3 8 8 8 8 8 8 8 2\n",
      " 8 8 8 8 8 8 8 1 8 8 8 8 4 3 8 8 8 5 8 8 8 8 8 8 8 3 2 8 1 8 8 8 8 8 8 8 8\n",
      " 5 8 8 2 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 8 8 8 1 8 8 8 0 8 8 2 8 8 8 8 8 5 8\n",
      " 0 8 8 8 8 8 8 8 5 1 1 8 8 8 8 8 8 8 8 8 6 8 8 8 0 8 8 8 8 8 3 8 8 3 8 8 8\n",
      " 8 8 2 8 8 8 8 8 2 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 8 8 3 8 8 6 2 8 8 8 8 1 8\n",
      " 8 8 8 4 1 8 8 8 8 2 2 8 8 8 2 8 3 8 8 8 8 8 8 8 8 0 8 8 8 8 8 8 1 3 8 8 8\n",
      " 8 2 2 1 8 8 8 4 8 2 8 2 8 4 2 1 8 8 8 8 8 8 2 4 8 8 8 8 8 2 8 3 8 1 8 2 2\n",
      " 8 8 8 2 4 8 7 5 2 8 8 8 5 8 8 8 8 8 2 8 8 8 8 4 8 8 8 8 8 8 8 8 8 3 8 8 1\n",
      " 4 8 8 8 8 8 8 2 8 8 8 8 8 8 8 8 8 8 8 8 1 8 8 3 6 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 2 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 2 8 8 8 4 1 8 8 2 8 8 8 8 8\n",
      " 8 0 8 8 8 2 8 8 2 2 8 8 4 3 8 8 4 8 4 8 8 8 8 8 8 8 8 8 8 3 8 0 8 8 8 8 8\n",
      " 8 8 8 1 8 2 1 8 8 8 8 2 8 8 8 3 8 2 8 8 8 8 8 1 2 8 8 8 8 8 8 4 8 8 3 5 8\n",
      " 8 8 1 8 8 1 8 8 8 2 8 8 8 8 8 8 8 8 1 8 8 1 1 8 8 8 2 8 8 8 8 8 2 8 8 8 8\n",
      " 8 0 8 4 8 8 8 8 8 8 8 8 8 8 8 8 2 4 8 8 8 8 4 8 8 8 2 8 0 8 4 8 1 8 8 8 3\n",
      " 8 8 8 8 8 8 8 0 8 8 8 8 8 8 8 8 8 2 8 8 8 8 8 8 8 8 5 8 8 3 8 8 8 3 8 3 8\n",
      " 8 0 8 1 8 8 8 8 8 8 8 8 1 8 8 8 3 8 0 8 8 8 8 8 8 8 8 8 8 8 3 8 8 4 8 8 8\n",
      " 8 8 8 2 8 1 8 8 8 8 8 8 8 1 8 3 8 8 8 8 8 8 8 1 1 8 8 8 8 8 8 8 2 5 2 4 8\n",
      " 8 8 8 8 4 8 8 2 2 2 8 8 5 2 4 8 8 8 8 8 8 4 8 8 8 8 8 1 8 8 8 8 8 8 1 8 8\n",
      " 3 2 1 8 8 1 8 8 3 8 8 4 8 8 8 8 8 8 8 1 8 8 8 1 8 3 8 8 8 8 8 8 4 8 8 8 5\n",
      " 8 8 1 8 1 3 8 8 8 8 8 8 1 8 8 8 8 8 8 3 8 8 8 8 8 1 8 8 8 0 8 8 4 8 8 8 8\n",
      " 8 1 8 8 8 8 1 8 8 4 8 8 8 8 8 1 8 8 8 8 8 4 8 8 8 2 8 8 8 8 8 8 8 8 8 2 8\n",
      " 8 8 3 8 8 8 4 8 8 8 8 3 8 8 8 8 0 8 8 8 3 1 8 8 8 8 8 8 2 8 8 8 8 8 8 8 1\n",
      " 8 2 8]\n"
     ]
    }
   ],
   "source": [
    "values = array(df_train['Deck'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "# ara el one hhot encode es A(0), B, C(2), D, E(4), F, G(6), T(7) i X(8)\n",
    "df_train['Deck_A']=onehot_encoded[:,0]\n",
    "df_train['Deck_B']=onehot_encoded[:,1]\n",
    "df_train['Deck_C']=onehot_encoded[:,2]\n",
    "df_train['Deck_D']=onehot_encoded[:,3]\n",
    "df_train['Deck_E']=onehot_encoded[:,4]\n",
    "df_train['Deck_F']=onehot_encoded[:,5]\n",
    "df_train['Deck_G']=onehot_encoded[:,6]\n",
    "df_train['Deck_X']=onehot_encoded[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'E' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X' 'B' 'X' 'A' 'X' 'X' 'X' 'X' 'X' 'C' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'D' 'X' 'X' 'D' 'X' 'A' 'X' 'D' 'X' 'C' 'X' 'X' 'C'\n",
      " 'X' 'X' 'X' 'F' 'X' 'B' 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'C' 'C' 'X' 'X'\n",
      " 'X' 'D' 'C' 'C' 'X' 'C' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'X' 'B' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'F' 'X' 'X' 'A' 'X' 'C' 'X' 'X' 'G' 'C' 'X' 'X' 'X' 'C' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'X'\n",
      " 'X' 'X' 'E' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'D' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'C' 'F' 'E'\n",
      " 'X' 'E' 'D' 'X' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'E' 'X'\n",
      " 'X' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'A' 'X' 'X' 'X' 'X' 'X' 'X' 'B'\n",
      " 'X' 'X' 'C' 'X' 'X' 'X' 'F' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'C' 'X' 'D' 'X' 'X' 'C' 'X' 'X' 'E' 'X' 'X' 'D' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'C' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B'\n",
      " 'F' 'X' 'X' 'X' 'X' 'A' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'D' 'X' 'X' 'X' 'B'\n",
      " 'C' 'X' 'B' 'X' 'X' 'X' 'X' 'X' 'C' 'X' 'B' 'X' 'X' 'X' 'X' 'F' 'F' 'X'\n",
      " 'X' 'X' 'F' 'X' 'X' 'X' 'X' 'A' 'X' 'X' 'X' 'C' 'X' 'X' 'X' 'X' 'X' 'X'\n",
      " 'X' 'B' 'X' 'X' 'X' 'X' 'X' 'X' 'D' 'X' 'X' 'X' 'X' 'E' 'C' 'X' 'X' 'X'\n",
      " 'X' 'X' 'X' 'X' 'E' 'X' 'X' 'X' 'X' 'X' 'X' 'E' 'B' 'X' 'A' 'X' 'X' 'X'\n",
      " 'C' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'B' 'D' 'X' 'X' 'X' 'C'\n",
      " 'X' 'B' 'X' 'X' 'C' 'X' 'X' 'X' 'D' 'D' 'X' 'C' 'X' 'X' 'X' 'C' 'X' 'X'\n",
      " 'C' 'X' 'X' 'X']\n",
      "[7 7 7 7 7 7 7 7 7 7 7 7 1 7 4 7 7 7 7 7 7 7 7 7 1 7 1 7 0 7 7 7 7 7 2 7 7\n",
      " 7 7 7 7 3 7 7 3 7 0 7 3 7 2 7 7 2 7 7 7 5 7 1 7 7 7 7 1 7 7 7 2 2 7 7 7 3\n",
      " 2 2 7 2 7 7 7 2 7 7 7 7 7 7 7 7 7 7 1 7 7 7 2 7 7 7 2 7 7 7 7 7 7 7 7 5 7\n",
      " 7 0 7 2 7 7 6 2 7 7 7 2 7 7 7 7 7 7 7 7 2 7 7 7 7 7 7 7 7 7 7 1 7 7 7 4 7\n",
      " 7 7 2 7 7 7 7 7 2 7 3 7 7 7 7 7 7 7 1 7 7 7 7 7 7 7 7 7 7 2 5 4 7 4 3 7 1\n",
      " 7 7 7 7 7 7 7 7 7 7 7 4 7 7 7 7 7 2 7 7 7 7 7 0 7 7 7 7 7 7 1 7 7 2 7 7 7\n",
      " 5 7 7 7 7 7 7 7 7 7 7 7 2 7 3 7 7 2 7 7 4 7 7 3 7 7 7 7 7 7 2 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 2 7 2 7 7 7 7 7 7 7 7 7 7 7 7 7 7 1 5 7 7 7 7 0 7 7\n",
      " 7 7 7 7 7 3 7 7 7 1 2 7 1 7 7 7 7 7 2 7 1 7 7 7 7 5 5 7 7 7 5 7 7 7 7 0 7\n",
      " 7 7 2 7 7 7 7 7 7 7 1 7 7 7 7 7 7 3 7 7 7 7 4 2 7 7 7 7 7 7 7 4 7 7 7 7 7\n",
      " 7 4 1 7 0 7 7 7 2 7 7 7 7 7 7 7 7 7 7 7 1 3 7 7 7 2 7 1 7 7 2 7 7 7 3 3 7\n",
      " 2 7 7 7 2 7 7 2 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "values = array(df_test['Deck'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "# ara el one hhot encode es A(0), B, C(2), D, E(4), F, G(6), T(7) i X(8)\n",
    "df_test['Deck_A']=onehot_encoded[:,0]\n",
    "df_test['Deck_B']=onehot_encoded[:,1]\n",
    "df_test['Deck_C']=onehot_encoded[:,2]\n",
    "df_test['Deck_D']=onehot_encoded[:,3]\n",
    "df_test['Deck_E']=onehot_encoded[:,4]\n",
    "df_test['Deck_F']=onehot_encoded[:,5]\n",
    "df_test['Deck_G']=onehot_encoded[:,6]\n",
    "df_test['Deck_X']=onehot_encoded[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La part de la Cabin i la Deck ja està feta.\n",
    "\n",
    "# El seguent punt seria categoritzar edat? \n",
    "Resulta que en aquella època eren molt etsrictes amb el tema de dones i nens primer. Com categoritzem edat?\n",
    "1. Trams de 4 en 4? (sortirien uns 20 grups)\n",
    "2. o directament establir grups: 0-2 (bebe), 2-10(infant), 10-16 (nen), 16-20 (jove), 20-60 (Adult), 60-100 (Vell) i veure si hi ha relacio amb supervivencia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abans pero hem d'omplir les edats sense dades no?\n",
    "La Age te una rlacio important amb la Pclass i la SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_train['Age'][df_train['Age'].isnull()] = df_train['Age'].mean()\n",
    "\n",
    "df_test['Age'][df_test['Age'].isnull()] = df_test['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en aquest bloc definim una columna categoritzant les edats\n",
    "bins= [0,2,10,16,20,60,100]\n",
    "labels = ['bebe','infant','nen','jove','adult','vell']\n",
    "df_train['AgeGroup'] = pd.cut(df_train['Age'], bins=bins, labels=labels, right=False)\n",
    "df_test['AgeGroup'] = pd.cut(df_test['Age'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'infant' 'adult'\n",
      " 'nen' 'infant' 'adult' 'adult' 'adult' 'nen' 'adult' 'infant' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'nen' 'adult' 'infant' 'adult' 'adult'\n",
      " 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'jove' 'nen' 'adult' 'adult' 'adult' 'infant' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'jove' 'infant' 'adult' 'adult' 'adult'\n",
      " 'vell' 'adult' 'adult' 'adult' 'infant' 'nen' 'adult' 'adult' 'adult'\n",
      " 'infant' 'adult' 'adult' 'adult' 'jove' 'jove' 'adult' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'bebe' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'jove' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'nen' 'adult' 'adult' 'jove' 'adult' 'vell'\n",
      " 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult' 'adult' 'nen'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'jove' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'jove'\n",
      " 'jove' 'jove' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'jove' 'bebe' 'infant' 'adult' 'adult' 'adult' 'adult' 'vell'\n",
      " 'infant' 'bebe' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'infant' 'bebe' 'infant' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'jove' 'jove' 'infant' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'infant' 'adult'\n",
      " 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'jove' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'infant'\n",
      " 'adult' 'adult' 'adult' 'infant' 'jove' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'vell' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'infant' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult' 'infant'\n",
      " 'adult' 'vell' 'adult' 'jove' 'jove' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'infant' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'bebe'\n",
      " 'adult' 'jove' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'vell' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'infant' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'infant' 'adult' 'adult'\n",
      " 'adult' 'nen' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'jove' 'jove' 'adult' 'infant' 'adult' 'adult' 'adult'\n",
      " 'adult' 'jove' 'adult' 'bebe' 'adult' 'adult' 'adult' 'jove' 'bebe'\n",
      " 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'jove' 'adult' 'nen' 'adult' 'adult' 'adult'\n",
      " 'adult' 'jove' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'jove' 'adult' 'nen' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'infant' 'nen' 'adult' 'infant' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'bebe' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'infant' 'infant' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'vell' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'jove' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'infant'\n",
      " 'adult' 'jove' 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'infant' 'nen' 'adult' 'adult' 'vell' 'jove' 'adult' 'adult'\n",
      " 'infant' 'jove' 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'vell' 'adult' 'adult' 'adult' 'jove' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'jove' 'adult' 'vell' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'infant' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult' 'adult' 'adult'\n",
      " 'vell' 'adult' 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'infant' 'adult' 'bebe' 'adult' 'jove' 'adult'\n",
      " 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'jove' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'jove' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'nen'\n",
      " 'vell' 'adult' 'nen' 'jove' 'jove' 'nen' 'adult' 'infant' 'adult' 'adult'\n",
      " 'vell' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'infant'\n",
      " 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'nen' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell' 'jove' 'adult'\n",
      " 'jove' 'adult' 'infant' 'infant' 'adult' 'adult' 'adult' 'bebe' 'adult'\n",
      " 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'jove' 'adult' 'infant' 'adult' 'adult' 'nen' 'jove' 'adult' 'adult'\n",
      " 'adult' 'adult' 'jove' 'infant' 'bebe' 'adult' 'adult' 'jove' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'nen' 'bebe' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult' 'adult' 'nen'\n",
      " 'adult' 'adult' 'adult' 'adult' 'infant' 'adult' 'adult' 'bebe' 'adult'\n",
      " 'vell' 'nen' 'bebe' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'jove' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'infant' 'vell' 'infant' 'jove' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'nen' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult']\n",
      "[0 0 0 0 0 0 0 2 0 4 2 0 0 0 4 0 2 0 0 0 0 0 4 0 2 0 0 3 0 0 0 0 0 5 0 0 0\n",
      " 0 3 4 0 0 0 2 3 0 0 0 0 3 2 0 0 0 5 0 0 0 2 4 0 0 0 2 0 0 0 3 3 0 0 3 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 3 0 3 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 4 0 0 3 0 5 0 0 2 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 3 0 3 0 0 0 0 3 3 3 0 2\n",
      " 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 3 1 2 0 0 0 0 5 2 1 0 0 3 0 0 0 0 0 0 2 1 2\n",
      " 0 0 0 0 0 0 3 3 2 0 0 0 0 0 0 0 0 0 0 3 2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 3 0\n",
      " 0 0 0 0 3 0 3 0 0 0 0 2 0 0 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0\n",
      " 0 0 2 0 0 0 0 3 0 0 0 0 0 0 0 0 5 0 0 2 0 5 0 3 3 0 0 0 0 0 0 0 3 0 0 0 0\n",
      " 0 2 0 0 0 0 3 0 0 1 0 3 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 3 0 0 0\n",
      " 3 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0\n",
      " 0 3 3 0 2 0 0 0 0 3 0 1 0 0 0 3 1 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 2 0 0 0 0 0 0 0 0 0 3 0 4 0 0 0 0 3 0 0 3 0 0 0 0 0 3 0 4 0 0 5 0 0 0 0 0\n",
      " 0 2 4 0 2 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 2\n",
      " 0 0 5 0 0 0 0 0 2 0 0 0 5 0 0 0 0 0 0 3 0 0 0 3 3 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3 0 0 2 0 0 0 0 0 2 4 0 0 5 3 0 0 2 3 0 0 0 0\n",
      " 5 0 0 0 0 0 0 0 0 0 0 3 0 0 0 5 0 0 0 3 3 0 0 0 0 0 0 0 0 0 3 0 5 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 5 0 0 0\n",
      " 0 5 0 0 0 2 0 0 0 0 0 0 0 2 0 1 0 3 0 0 0 0 3 0 0 3 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 5 0 0 3 0 3 0 0 0 0 0 4 5 0 4 3 3 4 0 2 0 0 5 0 0 0 0 0 3 0 3\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 2 3 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 5 3 0 3 0 2 2 0 0 0 1 0 3 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 3 0\n",
      " 2 0 0 4 3 0 0 0 0 3 2 1 0 0 3 0 0 0 0 0 0 0 0 0 0 4 1 0 0 0 3 0 0 0 0 0 2\n",
      " 0 0 0 0 0 4 0 0 0 0 2 0 0 1 0 5 4 1 0 0 3 0 0 0 0 0 0 3 0 0 3 0 0 0 0 0 2\n",
      " 5 2 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 4 0 3 0 0 0 0 0 0 0 0 0 3\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# ara tornem a fer un hot encoding pels grups d'edats\n",
    "values = array(df_train['AgeGroup'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_train['adult']=onehot_encoded[:,0]\n",
    "df_train['bebe']=onehot_encoded[:,1]\n",
    "df_train['infant']=onehot_encoded[:,2]\n",
    "df_train['jove']=onehot_encoded[:,3]\n",
    "df_train['nen']=onehot_encoded[:,4]\n",
    "df_train['vell']=onehot_encoded[:,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llegenda --> adult:0, bebe:1 , infant:2, jove=3, nen=4,  vell=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adult' 'adult' 'vell' 'adult' 'adult' 'nen' 'adult' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'vell' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'nen' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'jove'\n",
      " 'adult' 'nen' 'adult' 'jove' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'infant'\n",
      " 'vell' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'infant'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'adult' 'jove' 'adult' 'adult' 'adult' 'jove' 'vell' 'jove' 'adult'\n",
      " 'bebe' 'adult' 'adult' 'nen' 'adult' 'adult' 'adult' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'nen' 'adult' 'vell' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell'\n",
      " 'adult' 'nen' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'infant'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'nen' 'adult' 'adult' 'adult' 'vell'\n",
      " 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult'\n",
      " 'adult' 'adult' 'adult' 'nen' 'vell' 'infant' 'adult' 'infant' 'jove'\n",
      " 'adult' 'adult' 'adult' 'bebe' 'adult' 'infant' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'vell' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'nen' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'vell' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'bebe' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult'\n",
      " 'adult' 'adult' 'bebe' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'bebe' 'adult' 'infant' 'infant' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'bebe'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'vell'\n",
      " 'adult' 'bebe' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult'\n",
      " 'jove' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'nen' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'jove' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult'\n",
      " 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'jove' 'adult' 'bebe' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'nen' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'infant' 'adult' 'adult' 'adult' 'jove' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'infant' 'adult' 'adult' 'nen' 'adult' 'adult' 'jove'\n",
      " 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'adult' 'jove' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult' 'infant' 'adult' 'adult' 'adult' 'adult'\n",
      " 'adult' 'adult' 'adult' 'adult']\n",
      "[0 0 5 0 0 4 0 0 3 0 0 0 0 5 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 4 0 0 0 0 3 0 3 0 4 0 3 0 0 5 0 0 0 0\n",
      " 0 0 0 0 0 0 2 5 0 0 0 0 0 3 0 2 0 0 0 0 0 0 5 0 0 0 0 0 0 0 3 0 0 0 0 3 0\n",
      " 0 0 3 5 3 0 1 0 0 4 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 5 0 0 0 0 0\n",
      " 0 0 0 0 5 0 4 0 0 0 0 0 0 2 0 0 0 0 0 3 0 0 0 0 0 0 0 4 0 0 0 5 0 0 3 0 0\n",
      " 0 0 3 0 0 0 0 4 5 2 0 2 3 0 0 0 1 0 2 0 0 0 0 0 0 0 0 3 5 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 4 0 0 0 0 0 5 0 3 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3\n",
      " 0 0 0 0 1 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 1 0 2 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 5 0 1 0 0 3 0 0 0 0 3 0 3 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0\n",
      " 3 0 0 0 0 0 0 3 0 0 0 0 3 0 0 0 0 0 0 3 0 1 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 3 0 0 0 0 0 2 0 0 4 0 0 3 0 0 0 0 0 0 0 3 0 0 0\n",
      " 0 0 2 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# ara tornem a fer un hot encoding pels grups d'edats\n",
    "values = array(df_test['AgeGroup'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_test['adult']=onehot_encoded[:,0]\n",
    "df_test['bebe']=onehot_encoded[:,1]\n",
    "df_test['infant']=onehot_encoded[:,2]\n",
    "df_test['jove']=onehot_encoded[:,3]\n",
    "df_test['nen']=onehot_encoded[:,4]\n",
    "df_test['vell']=onehot_encoded[:,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ara faria el hot encoding pel Pclass, Sexe, fare, SibSp i Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female' 'female' 'female' 'male' 'male' 'male' 'male' 'female'\n",
      " 'female' 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'male'\n",
      " 'female' 'female' 'male' 'male' 'female' 'male' 'female' 'female' 'male'\n",
      " 'male' 'female' 'male' 'male' 'female' 'female' 'male' 'male' 'male'\n",
      " 'male' 'male' 'female' 'female' 'female' 'female' 'male' 'female'\n",
      " 'female' 'male' 'male' 'female' 'male' 'female' 'male' 'male' 'female'\n",
      " 'female' 'male' 'male' 'female' 'male' 'female' 'male' 'male' 'female'\n",
      " 'male' 'male' 'male' 'male' 'female' 'male' 'female' 'male' 'male'\n",
      " 'female' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'female' 'male'\n",
      " 'male' 'female' 'male' 'female' 'female' 'male' 'male' 'female' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'female' 'male'\n",
      " 'female' 'male' 'male' 'male' 'male' 'male' 'female' 'male' 'male'\n",
      " 'female' 'male' 'female' 'male' 'female' 'female' 'male' 'male' 'male'\n",
      " 'male' 'female' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'female'\n",
      " 'male' 'male' 'male' 'female' 'female' 'female' 'male' 'male' 'male'\n",
      " 'male' 'female' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'female' 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'female' 'female' 'male' 'male' 'male' 'male' 'female' 'male' 'male'\n",
      " 'male' 'male' 'female' 'male' 'male' 'female' 'male' 'male' 'male'\n",
      " 'female' 'male' 'female' 'male' 'male' 'male' 'female' 'male' 'female'\n",
      " 'male' 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'male'\n",
      " 'male' 'male' 'male' 'female' 'male' 'male' 'female' 'male' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'female' 'male' 'female' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'female'\n",
      " 'female' 'male' 'male' 'female' 'male' 'female' 'male' 'female' 'male'\n",
      " 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female' 'female'\n",
      " 'male' 'male' 'male' 'female' 'male' 'male' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'male' 'male' 'male' 'male' 'female' 'male'\n",
      " 'male' 'male' 'female' 'female' 'male' 'male' 'female' 'male' 'female'\n",
      " 'female' 'female' 'male' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'female' 'female' 'female' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'male' 'female' 'female' 'male'\n",
      " 'male' 'female' 'male' 'male' 'female' 'female' 'male' 'female' 'female'\n",
      " 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'female'\n",
      " 'female' 'male' 'male' 'female' 'female' 'male' 'female' 'male' 'female'\n",
      " 'female' 'female' 'female' 'male' 'male' 'male' 'female' 'male' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'female'\n",
      " 'female' 'female' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'female' 'female' 'female' 'female' 'male' 'male' 'female' 'male' 'male'\n",
      " 'male' 'female' 'female' 'female' 'female' 'male' 'male' 'male' 'male'\n",
      " 'female' 'female' 'female' 'male' 'male' 'male' 'female' 'female' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'male' 'female' 'male' 'male'\n",
      " 'male' 'female' 'female' 'male' 'female' 'male' 'male' 'female' 'male'\n",
      " 'male' 'female' 'male' 'female' 'male' 'male' 'male' 'male' 'female'\n",
      " 'male' 'male' 'female' 'male' 'male' 'female' 'female' 'female' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'male' 'male' 'female' 'female'\n",
      " 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'female' 'female'\n",
      " 'female' 'male' 'male' 'female' 'male' 'male' 'female' 'male' 'male'\n",
      " 'female' 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'female' 'male' 'male' 'female' 'female' 'female'\n",
      " 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'female' 'male'\n",
      " 'female' 'female' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'female' 'male' 'female' 'male' 'male' 'female' 'female' 'female'\n",
      " 'female' 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'female' 'male' 'male' 'female' 'male' 'female' 'male' 'female' 'male'\n",
      " 'male' 'female' 'male' 'male' 'female' 'male' 'male' 'male' 'female'\n",
      " 'male' 'male' 'female' 'female' 'female' 'male' 'female' 'male' 'female'\n",
      " 'female' 'female' 'female' 'male' 'male' 'male' 'female' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'female' 'male' 'female' 'male'\n",
      " 'female' 'female' 'male' 'male' 'male' 'male' 'female' 'male' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'male' 'female' 'male' 'male'\n",
      " 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'male' 'male'\n",
      " 'female' 'male' 'male' 'male' 'male' 'male' 'female' 'male' 'female'\n",
      " 'male' 'male' 'female' 'male' 'male' 'male' 'female' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'female' 'female' 'female' 'male' 'female'\n",
      " 'male' 'male' 'female' 'male' 'female' 'female' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'female' 'female' 'male' 'male' 'female' 'male' 'male'\n",
      " 'female' 'female' 'male' 'female' 'male' 'male' 'male' 'male' 'female'\n",
      " 'male' 'female' 'male' 'female' 'female' 'male' 'male' 'female' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'female' 'female' 'male' 'male' 'male' 'male' 'male' 'male' 'female'\n",
      " 'female' 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'female' 'male' 'female' 'male' 'male' 'male' 'male' 'male'\n",
      " 'female' 'male' 'male' 'female' 'male' 'female' 'male' 'male' 'male'\n",
      " 'female' 'male' 'female' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'male' 'female' 'female' 'male' 'male' 'female' 'male' 'male' 'male'\n",
      " 'male' 'male' 'female' 'female' 'male' 'female' 'female' 'male' 'male'\n",
      " 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'female'\n",
      " 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'female' 'male' 'male'\n",
      " 'male' 'female' 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male'\n",
      " 'female' 'male' 'female' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'female' 'male' 'female' 'male' 'male' 'female' 'male' 'female' 'female'\n",
      " 'female' 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'male' 'female' 'male' 'male' 'male' 'female' 'female' 'male' 'female'\n",
      " 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'female' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'male' 'male' 'female' 'male'\n",
      " 'male' 'male' 'female' 'male' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'female' 'male' 'male' 'female' 'female' 'female' 'female'\n",
      " 'female' 'male' 'female' 'male' 'male' 'male' 'female' 'female' 'male'\n",
      " 'female' 'female' 'male' 'male' 'male' 'male' 'female' 'male' 'male'\n",
      " 'female' 'female' 'male' 'male' 'male' 'female' 'female' 'male' 'female'\n",
      " 'male' 'male' 'female' 'male' 'female' 'female' 'male' 'male']\n",
      "[1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1\n",
      " 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
      " 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1\n",
      " 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1\n",
      " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
      " 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1\n",
      " 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
      " 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# ---------SEX---\n",
    "values = array(df_train['Sex'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_train['female']=onehot_encoded[:,0]\n",
    "df_train['male']=onehot_encoded[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female' 'male' 'male' 'female' 'male' 'female' 'male' 'female'\n",
      " 'male' 'male' 'male' 'female' 'male' 'female' 'female' 'male' 'male'\n",
      " 'female' 'female' 'male' 'male' 'female' 'male' 'female' 'male' 'female'\n",
      " 'male' 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'male'\n",
      " 'female' 'female' 'male' 'male' 'male' 'male' 'male' 'female' 'female'\n",
      " 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'female' 'female'\n",
      " 'male' 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'female'\n",
      " 'male' 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'female'\n",
      " 'male' 'female' 'male' 'male' 'female' 'male' 'female' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'female' 'female' 'female' 'male' 'female'\n",
      " 'male' 'female' 'male' 'male' 'male' 'female' 'male' 'female' 'male'\n",
      " 'female' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'female' 'female' 'female' 'female' 'male' 'male' 'female' 'male'\n",
      " 'female' 'female' 'male' 'female' 'male' 'male' 'female' 'male' 'female'\n",
      " 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male' 'male'\n",
      " 'female' 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'female' 'male' 'male' 'female' 'male' 'male'\n",
      " 'female' 'female' 'male' 'female' 'female' 'male' 'female' 'male' 'male'\n",
      " 'female' 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'male'\n",
      " 'male' 'female' 'female' 'male' 'female' 'female' 'male' 'male' 'female'\n",
      " 'male' 'female' 'male' 'female' 'male' 'female' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'female' 'male' 'female' 'female'\n",
      " 'male' 'male' 'female' 'male' 'male' 'female' 'male' 'female' 'male'\n",
      " 'male' 'male' 'male' 'female' 'female' 'male' 'female' 'male' 'female'\n",
      " 'male' 'female' 'male' 'female' 'male' 'female' 'female' 'male' 'female'\n",
      " 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'female' 'female' 'female' 'female' 'male' 'male' 'male' 'male' 'female'\n",
      " 'male' 'female' 'female' 'female' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'female' 'male' 'male' 'male' 'female' 'female' 'male'\n",
      " 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'female' 'female'\n",
      " 'male' 'female' 'male' 'male' 'male' 'male' 'female' 'male' 'female'\n",
      " 'female' 'female' 'male' 'male' 'male' 'male' 'male' 'male' 'female'\n",
      " 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'female' 'male'\n",
      " 'male' 'male' 'female' 'female' 'female' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'female' 'male' 'female' 'male' 'male' 'male'\n",
      " 'female' 'male' 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'female' 'male' 'female' 'male' 'female' 'male'\n",
      " 'female' 'female' 'male' 'male' 'male' 'female' 'male' 'female' 'male'\n",
      " 'male' 'female' 'male' 'female' 'female' 'male' 'female' 'female' 'male'\n",
      " 'female' 'female' 'male' 'male' 'female' 'male' 'male' 'female' 'female'\n",
      " 'female' 'male' 'male' 'male' 'male' 'male' 'female' 'female' 'male'\n",
      " 'female' 'male' 'male' 'male' 'male' 'male' 'female' 'male' 'male' 'male'\n",
      " 'female' 'male' 'female' 'male' 'male' 'female' 'male' 'female' 'male'\n",
      " 'male' 'male' 'male' 'male' 'female' 'female' 'female' 'female' 'female'\n",
      " 'male' 'female' 'male' 'male' 'male']\n",
      "[1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1\n",
      " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1\n",
      " 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0\n",
      " 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
      " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# ---------SEX---\n",
    "values = array(df_test['Sex'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_test['female']=onehot_encoded[:,0]\n",
    "df_test['male']=onehot_encoded[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 3 1 3 3 1 3 3 2 3 1 3 3 3 2 3 2 3 3 2 2 3 1 3 3 3 1 3 3 1 1 3 2 1 1 3\n",
      " 3 3 3 3 2 3 2 3 3 3 3 3 3 3 3 1 2 1 1 2 3 2 3 3 1 1 3 1 3 2 3 3 3 2 3 2 3\n",
      " 3 3 3 3 2 3 3 3 3 1 2 3 3 3 1 3 3 3 1 3 3 3 1 1 2 2 3 3 1 3 3 3 3 3 3 3 1\n",
      " 3 3 3 3 3 3 2 1 3 2 3 2 2 1 3 3 3 3 3 3 3 3 2 2 2 1 1 3 1 3 3 3 3 2 2 3 3\n",
      " 2 2 2 1 3 3 3 1 3 3 3 3 3 2 3 3 3 3 1 3 1 3 1 3 3 3 1 3 3 1 2 3 3 2 3 2 3\n",
      " 1 3 1 3 3 2 2 3 2 1 1 3 3 3 2 3 3 3 3 3 3 3 3 3 1 3 2 3 2 3 1 3 2 1 2 3 2\n",
      " 3 3 1 3 2 3 2 3 1 3 2 3 2 3 2 2 2 2 3 3 2 3 3 1 3 2 1 2 3 3 1 3 3 3 1 1 1\n",
      " 2 3 3 1 1 3 2 3 3 1 1 1 3 2 1 3 1 3 2 3 3 3 3 3 3 1 3 3 3 2 3 1 1 2 3 3 1\n",
      " 3 1 1 1 3 3 3 2 3 1 1 1 2 1 1 1 2 3 2 3 2 2 1 1 3 3 2 2 3 1 3 2 3 1 3 1 1\n",
      " 3 1 3 1 1 3 1 2 1 2 2 2 2 2 3 3 3 3 1 3 3 3 3 1 2 3 3 3 2 3 3 3 3 1 3 3 1\n",
      " 1 3 3 1 3 1 3 1 3 3 1 3 3 1 3 2 3 2 3 2 1 3 3 1 3 3 3 2 2 2 3 3 3 3 3 2 3\n",
      " 2 3 3 3 3 1 2 3 3 2 2 2 3 3 3 3 3 3 3 2 2 3 3 1 3 2 3 1 1 3 2 1 2 2 3 3 2\n",
      " 3 1 2 1 3 1 2 3 1 1 3 3 1 1 2 3 1 3 1 2 3 3 2 1 3 3 3 3 2 2 3 1 2 3 3 3 3\n",
      " 2 3 3 1 3 1 1 3 3 3 3 1 1 3 3 1 3 1 3 3 3 3 3 1 1 2 1 3 3 3 3 1 1 3 1 2 3\n",
      " 2 3 1 3 3 1 3 3 2 1 3 2 2 3 3 3 3 2 1 1 3 1 1 3 3 2 1 1 2 2 3 2 1 2 3 3 3\n",
      " 1 1 1 1 3 3 3 2 3 3 3 3 3 3 3 2 1 1 3 3 3 2 1 3 3 2 1 2 1 3 1 2 1 3 3 3 1\n",
      " 3 3 2 3 2 3 3 1 2 3 1 3 1 3 3 1 2 1 3 3 3 3 3 2 3 3 2 2 3 1 3 3 3 1 2 1 3\n",
      " 3 1 3 1 1 3 2 3 2 3 3 3 1 3 3 3 1 3 1 3 3 3 2 3 3 3 2 3 3 2 1 1 3 1 3 3 2\n",
      " 2 3 3 1 2 1 2 2 2 3 3 3 3 1 3 1 3 3 2 2 3 3 3 1 1 3 3 3 1 2 3 3 1 3 1 1 3\n",
      " 3 3 2 2 1 1 3 1 1 1 3 2 3 1 2 3 3 2 3 2 2 1 3 2 3 2 3 1 3 2 2 2 3 3 1 3 3\n",
      " 1 1 1 3 3 1 3 2 1 3 2 3 3 3 2 2 3 2 3 1 3 3 3 1 3 1 1 3 3 3 3 3 2 3 2 3 3\n",
      " 3 3 1 3 1 1 3 3 3 3 3 3 1 3 2 3 1 3 2 1 3 3 3 2 2 1 3 3 3 1 3 2 1 3 3 2 3\n",
      " 3 1 3 2 3 3 1 3 1 3 3 3 3 2 3 1 3 2 3 3 3 1 3 3 3 1 3 2 1 3 3 3 3 3 2 1 3\n",
      " 3 3 1 2 3 1 1 3 3 3 2 1 3 2 2 2 1 3 3 3 1 1 3 2 3 3 3 3 1 2 3 3 2 3 3 2 1\n",
      " 3 1 3]\n",
      "[2 0 2 0 2 2 0 2 2 1 2 0 2 2 2 1 2 1 2 2 1 1 2 0 2 2 2 0 2 2 0 0 2 1 0 0 2\n",
      " 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 0 1 0 0 1 2 1 2 2 0 0 2 0 2 1 2 2 2 1 2 1 2\n",
      " 2 2 2 2 1 2 2 2 2 0 1 2 2 2 0 2 2 2 0 2 2 2 0 0 1 1 2 2 0 2 2 2 2 2 2 2 0\n",
      " 2 2 2 2 2 2 1 0 2 1 2 1 1 0 2 2 2 2 2 2 2 2 1 1 1 0 0 2 0 2 2 2 2 1 1 2 2\n",
      " 1 1 1 0 2 2 2 0 2 2 2 2 2 1 2 2 2 2 0 2 0 2 0 2 2 2 0 2 2 0 1 2 2 1 2 1 2\n",
      " 0 2 0 2 2 1 1 2 1 0 0 2 2 2 1 2 2 2 2 2 2 2 2 2 0 2 1 2 1 2 0 2 1 0 1 2 1\n",
      " 2 2 0 2 1 2 1 2 0 2 1 2 1 2 1 1 1 1 2 2 1 2 2 0 2 1 0 1 2 2 0 2 2 2 0 0 0\n",
      " 1 2 2 0 0 2 1 2 2 0 0 0 2 1 0 2 0 2 1 2 2 2 2 2 2 0 2 2 2 1 2 0 0 1 2 2 0\n",
      " 2 0 0 0 2 2 2 1 2 0 0 0 1 0 0 0 1 2 1 2 1 1 0 0 2 2 1 1 2 0 2 1 2 0 2 0 0\n",
      " 2 0 2 0 0 2 0 1 0 1 1 1 1 1 2 2 2 2 0 2 2 2 2 0 1 2 2 2 1 2 2 2 2 0 2 2 0\n",
      " 0 2 2 0 2 0 2 0 2 2 0 2 2 0 2 1 2 1 2 1 0 2 2 0 2 2 2 1 1 1 2 2 2 2 2 1 2\n",
      " 1 2 2 2 2 0 1 2 2 1 1 1 2 2 2 2 2 2 2 1 1 2 2 0 2 1 2 0 0 2 1 0 1 1 2 2 1\n",
      " 2 0 1 0 2 0 1 2 0 0 2 2 0 0 1 2 0 2 0 1 2 2 1 0 2 2 2 2 1 1 2 0 1 2 2 2 2\n",
      " 1 2 2 0 2 0 0 2 2 2 2 0 0 2 2 0 2 0 2 2 2 2 2 0 0 1 0 2 2 2 2 0 0 2 0 1 2\n",
      " 1 2 0 2 2 0 2 2 1 0 2 1 1 2 2 2 2 1 0 0 2 0 0 2 2 1 0 0 1 1 2 1 0 1 2 2 2\n",
      " 0 0 0 0 2 2 2 1 2 2 2 2 2 2 2 1 0 0 2 2 2 1 0 2 2 1 0 1 0 2 0 1 0 2 2 2 0\n",
      " 2 2 1 2 1 2 2 0 1 2 0 2 0 2 2 0 1 0 2 2 2 2 2 1 2 2 1 1 2 0 2 2 2 0 1 0 2\n",
      " 2 0 2 0 0 2 1 2 1 2 2 2 0 2 2 2 0 2 0 2 2 2 1 2 2 2 1 2 2 1 0 0 2 0 2 2 1\n",
      " 1 2 2 0 1 0 1 1 1 2 2 2 2 0 2 0 2 2 1 1 2 2 2 0 0 2 2 2 0 1 2 2 0 2 0 0 2\n",
      " 2 2 1 1 0 0 2 0 0 0 2 1 2 0 1 2 2 1 2 1 1 0 2 1 2 1 2 0 2 1 1 1 2 2 0 2 2\n",
      " 0 0 0 2 2 0 2 1 0 2 1 2 2 2 1 1 2 1 2 0 2 2 2 0 2 0 0 2 2 2 2 2 1 2 1 2 2\n",
      " 2 2 0 2 0 0 2 2 2 2 2 2 0 2 1 2 0 2 1 0 2 2 2 1 1 0 2 2 2 0 2 1 0 2 2 1 2\n",
      " 2 0 2 1 2 2 0 2 0 2 2 2 2 1 2 0 2 1 2 2 2 0 2 2 2 0 2 1 0 2 2 2 2 2 1 0 2\n",
      " 2 2 0 1 2 0 0 2 2 2 1 0 2 1 1 1 0 2 2 2 0 0 2 1 2 2 2 2 0 1 2 2 1 2 2 1 0\n",
      " 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "# ---------Pclass---\n",
    "values = array(df_train['Pclass'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_train['Pclass1']=onehot_encoded[:,0]\n",
    "df_train['Pclass2']=onehot_encoded[:,1]\n",
    "df_train['Pclass3']=onehot_encoded[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 3 3 3 3 2 3 3 3 1 1 2 1 2 2 3 3 3 1 3 1 1 1 3 1 3 1 3 2 2 3 3 1 3 3\n",
      " 3 3 3 3 1 3 2 1 3 1 3 1 3 1 2 2 1 2 3 3 3 3 1 3 2 3 3 1 2 3 1 1 1 3 3 3 1\n",
      " 1 1 3 1 2 3 3 1 1 3 2 3 3 3 3 2 3 3 1 3 1 3 1 3 3 3 1 2 3 3 3 3 3 3 3 2 2\n",
      " 3 1 3 1 3 3 3 1 2 2 3 1 3 3 3 3 3 2 3 3 1 3 3 3 3 3 2 3 3 3 1 1 2 1 3 1 3\n",
      " 1 2 1 3 3 3 3 3 1 3 1 3 3 3 2 3 2 3 1 3 1 3 3 3 3 3 3 2 2 1 2 1 2 1 1 3 1\n",
      " 2 2 3 3 2 2 1 3 2 2 3 1 3 2 3 3 3 1 2 2 1 3 2 1 3 3 3 2 2 3 1 3 1 1 3 2 3\n",
      " 2 3 1 3 3 3 3 2 2 1 3 3 1 3 1 3 2 1 1 2 1 3 3 1 2 2 2 3 2 3 1 3 3 3 3 3 2\n",
      " 3 3 3 2 3 2 3 1 3 3 3 1 3 1 3 3 2 2 2 2 2 3 3 3 3 3 3 3 1 3 3 1 3 3 1 3 3\n",
      " 2 3 1 3 3 2 2 3 3 1 1 3 1 3 3 3 3 3 1 3 1 2 3 2 3 3 2 1 1 3 2 1 2 2 2 1 3\n",
      " 3 3 1 2 3 2 3 2 3 3 1 3 3 2 3 2 2 1 2 2 2 3 1 1 3 3 3 3 2 2 3 1 3 3 3 1 2\n",
      " 2 1 1 2 1 1 3 2 1 3 3 3 3 3 2 2 3 2 3 3 1 1 3 2 3 1 3 1 3 3 1 2 1 1 1 2 2\n",
      " 1 3 3 3 1 3 3 1 3 3 3]\n",
      "[2 2 1 2 2 2 2 1 2 2 2 0 0 1 0 1 1 2 2 2 0 2 0 0 0 2 0 2 0 2 1 1 2 2 0 2 2\n",
      " 2 2 2 2 0 2 1 0 2 0 2 0 2 0 1 1 0 1 2 2 2 2 0 2 1 2 2 0 1 2 0 0 0 2 2 2 0\n",
      " 0 0 2 0 1 2 2 0 0 2 1 2 2 2 2 1 2 2 0 2 0 2 0 2 2 2 0 1 2 2 2 2 2 2 2 1 1\n",
      " 2 0 2 0 2 2 2 0 1 1 2 0 2 2 2 2 2 1 2 2 0 2 2 2 2 2 1 2 2 2 0 0 1 0 2 0 2\n",
      " 0 1 0 2 2 2 2 2 0 2 0 2 2 2 1 2 1 2 0 2 0 2 2 2 2 2 2 1 1 0 1 0 1 0 0 2 0\n",
      " 1 1 2 2 1 1 0 2 1 1 2 0 2 1 2 2 2 0 1 1 0 2 1 0 2 2 2 1 1 2 0 2 0 0 2 1 2\n",
      " 1 2 0 2 2 2 2 1 1 0 2 2 0 2 0 2 1 0 0 1 0 2 2 0 1 1 1 2 1 2 0 2 2 2 2 2 1\n",
      " 2 2 2 1 2 1 2 0 2 2 2 0 2 0 2 2 1 1 1 1 1 2 2 2 2 2 2 2 0 2 2 0 2 2 0 2 2\n",
      " 1 2 0 2 2 1 1 2 2 0 0 2 0 2 2 2 2 2 0 2 0 1 2 1 2 2 1 0 0 2 1 0 1 1 1 0 2\n",
      " 2 2 0 1 2 1 2 1 2 2 0 2 2 1 2 1 1 0 1 1 1 2 0 0 2 2 2 2 1 1 2 0 2 2 2 0 1\n",
      " 1 0 0 1 0 0 2 1 0 2 2 2 2 2 1 1 2 1 2 2 0 0 2 1 2 0 2 0 2 2 0 1 0 0 0 1 1\n",
      " 0 2 2 2 0 2 2 0 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# ---------Pclass---\n",
    "values = array(df_test['Pclass'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_test['Pclass1']=onehot_encoded[:,0]\n",
    "df_test['Pclass2']=onehot_encoded[:,1]\n",
    "df_test['Pclass3']=onehot_encoded[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3 4 2 5 8]\n",
      "[1 1 0 1 0 0 0 3 0 1 1 0 0 1 0 0 4 0 1 0 0 0 0 0 3 1 0 3 0 0 0 1 0 0 1 1 0\n",
      " 0 2 1 1 1 0 1 0 0 1 0 2 1 4 0 1 1 0 0 0 0 1 5 0 0 1 3 0 1 0 0 4 2 0 5 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 3 1 0 3 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 1 0\n",
      " 1 0 1 0 0 0 1 0 4 2 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 2\n",
      " 0 0 0 1 0 0 0 0 0 0 0 8 0 0 0 0 4 0 0 1 0 0 0 4 1 0 0 1 3 0 0 0 8 0 4 2 0\n",
      " 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 8 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 3 1 0 0 4 0 0 1 0 0 0 1 1 0 0 0 2 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 0 0 4 1 0 0 0 4 1 0 0 0 0 0 0 0 1 0 0 4 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 2 0 0 0 1 0 1 1 0 0 2 1 0 1 0 1 0 0 1 0 0 0 1 8 0 0 0 1 0 2 0 0\n",
      " 2 1 0 1 0 0 0 1 3 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 3 1 0 0 0 0 0 0 0 1 0 0 5 0 0 0 1 0 2 1 0 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 1 0 3 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 2 2 1 0 1 0 1 0\n",
      " 0 0 0 0 2 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0 1 1 0 0 5\n",
      " 0 0 0 1 3 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 2 1 0 1 0 0 0 0 0 0 0 0 4 4 1 1 0 1 0 1 1 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 1 1 0 0 0 1 2 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 2 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 3 0 0 1 0 1 0 0 3 0 2 1 0 0 0 0 0 0 0 0 0 2 0 1 0 0 2 0 0 0 1 2\n",
      " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 5 1 1 4 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 3 0 1 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 2 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 4 1 0 0 0 8 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 4\n",
      " 0 0 0 1 0 3 1 0 0 0 4 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 8 0 0 1 4\n",
      " 0 1 0 1 0 1 0 0 0 2 1 0 8 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0]\n",
      "[1 1 0 1 0 0 0 3 0 1 1 0 0 1 0 0 4 0 1 0 0 0 0 0 3 1 0 3 0 0 0 1 0 0 1 1 0\n",
      " 0 2 1 1 1 0 1 0 0 1 0 2 1 4 0 1 1 0 0 0 0 1 5 0 0 1 3 0 1 0 0 4 2 0 5 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 3 1 0 3 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 1 0\n",
      " 1 0 1 0 0 0 1 0 4 2 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 2\n",
      " 0 0 0 1 0 0 0 0 0 0 0 6 0 0 0 0 4 0 0 1 0 0 0 4 1 0 0 1 3 0 0 0 6 0 4 2 0\n",
      " 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 6 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 3 1 0 0 4 0 0 1 0 0 0 1 1 0 0 0 2 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 0 0 4 1 0 0 0 4 1 0 0 0 0 0 0 0 1 0 0 4 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 2 0 0 0 1 0 1 1 0 0 2 1 0 1 0 1 0 0 1 0 0 0 1 6 0 0 0 1 0 2 0 0\n",
      " 2 1 0 1 0 0 0 1 3 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 3 1 0 0 0 0 0 0 0 1 0 0 5 0 0 0 1 0 2 1 0 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 1 0 3 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 2 2 1 0 1 0 1 0\n",
      " 0 0 0 0 2 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0 1 1 0 0 5\n",
      " 0 0 0 1 3 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 2 1 0 1 0 0 0 0 0 0 0 0 4 4 1 1 0 1 0 1 1 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 1 1 0 0 0 1 2 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 2 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 3 0 0 1 0 1 0 0 3 0 2 1 0 0 0 0 0 0 0 0 0 2 0 1 0 0 2 0 0 0 1 2\n",
      " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 5 1 1 4 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 3 0 1 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 2 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 4 1 0 0 0 6 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 4\n",
      " 0 0 0 1 0 3 1 0 0 0 4 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 6 0 0 1 4\n",
      " 0 1 0 1 0 1 0 0 0 2 1 0 6 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# ---------SibSP--- TRAIN\n",
    "del values\n",
    "print (df_train['SibSp'].unique())\n",
    "values = array(df_train['SibSp'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_train['SibSp0']=onehot_encoded[:,0]\n",
    "df_train['SibSp1']=onehot_encoded[:,1]\n",
    "df_train['SibSp2']=onehot_encoded[:,2]\n",
    "df_train['SibSp3']=onehot_encoded[:,3]\n",
    "df_train['SibSp4']=onehot_encoded[:,4]\n",
    "df_train['SibSp5']=onehot_encoded[:,5]\n",
    "df_train['SibSp8']=onehot_encoded[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 8]\n",
      "[0 1 0 0 1 0 0 1 0 2 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 2 1 2 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 2 3 0 4 0 0 1 0 0 0 0 0 2 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 2 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 1 5 0 1 0 0 3 0 0\n",
      " 0 1 0 0 0 0 4 0 0 0 0 0 0 1 0 0 0 1 0 2 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1\n",
      " 0 0 2 8 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 4 0 0 1 1 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
      " 1 2 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 2 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 2 0 0 1 8 1 0 0 1 1 1 0 0 0\n",
      " 1 0 0 0 1 0 2 0 0 4 0 0 0 1 0 1 0 0 0 3 0 0 0 0 3 1 0 1 0 0 0 1 0 0 1 0 1\n",
      " 1 0 1 0 1 0 0 0 0 0 1]\n",
      "[0 1 0 0 1 0 0 1 0 2 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 2 1 2 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 2 3 0 4 0 0 1 0 0 0 0 0 2 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 2 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 1 5 0 1 0 0 3 0 0\n",
      " 0 1 0 0 0 0 4 0 0 0 0 0 0 1 0 0 0 1 0 2 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1\n",
      " 0 0 2 6 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 4 0 0 1 1 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
      " 1 2 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 2 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 2 0 0 1 6 1 0 0 1 1 1 0 0 0\n",
      " 1 0 0 0 1 0 2 0 0 4 0 0 0 1 0 1 0 0 0 3 0 0 0 0 3 1 0 1 0 0 0 1 0 0 1 0 1\n",
      " 1 0 1 0 1 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# ---------SibSP--- TEST\n",
    "del values\n",
    "print (df_test['SibSp'].unique())\n",
    "values = array(df_test['SibSp'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_test['SibSp0']=onehot_encoded[:,0]\n",
    "df_test['SibSp1']=onehot_encoded[:,1]\n",
    "df_test['SibSp2']=onehot_encoded[:,2]\n",
    "df_test['SibSp3']=onehot_encoded[:,3]\n",
    "df_test['SibSp4']=onehot_encoded[:,4]\n",
    "df_test['SibSp5']=onehot_encoded[:,5]\n",
    "df_test['SibSp8']=onehot_encoded[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 5 3 4 6]\n",
      "[0 0 0 0 0 0 0 1 2 0 1 0 0 5 0 0 1 0 0 0 0 0 0 0 1 5 0 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 1 0 0 0 2 2 0 0 0 2 0 1 0 0 2 0 0 2 0 0\n",
      " 0 0 0 0 2 0 0 0 0 0 0 0 3 0 2 0 0 0 0 2 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 2 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 1 0 2\n",
      " 2 0 0 0 0 2 0 1 0 0 0 2 1 0 0 0 1 2 1 4 0 0 0 1 1 0 0 1 1 0 0 0 2 0 2 1 2\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 2 1 0 0 1 0 0 2 2 0 0 0\n",
      " 1 0 2 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 2 0 1 0 0 0 0 0 2 0 0 0 0 0 2 1 0 1 0 0 0 2 1 0 0 0 1 2 0 0 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 4 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 2 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 1 2 0 2 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 2 3 4 0 1 0 0 0\n",
      " 0 2 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 1 2\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 2 0 2 0 0 0 2 2 2 2 0 0 0 0 0 1 1 2 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 2 0 1 0 0 0 0\n",
      " 0 2 0 1 0 0 0 0 1 0 0 0 0 0 0 0 2 0 5 0 0 0 0 2 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 2 0 0 1 5 0 0 0 2 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 2 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 6 1 0 0 0 2 1 2 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0\n",
      " 0 0 2 0 0 1 1 0 0 0 1 1 0 0 2 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 3 0 0\n",
      " 0 0 1 0 0 0 2 0 0 0 1 2 0 0 0 2 0 0 0 0 0 0 1 0 1 2 1 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 1 0 2 1 0 0 1 1 0 0 2 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 1 0 2\n",
      " 0 1 1 0 1 1 0 3 0 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 5 0 0\n",
      " 2 0 0]\n",
      "[0 0 0 0 0 0 0 1 2 0 1 0 0 5 0 0 1 0 0 0 0 0 0 0 1 5 0 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 1 0 0 0 2 2 0 0 0 2 0 1 0 0 2 0 0 2 0 0\n",
      " 0 0 0 0 2 0 0 0 0 0 0 0 3 0 2 0 0 0 0 2 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 2 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 1 0 2\n",
      " 2 0 0 0 0 2 0 1 0 0 0 2 1 0 0 0 1 2 1 4 0 0 0 1 1 0 0 1 1 0 0 0 2 0 2 1 2\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 2 1 0 0 1 0 0 2 2 0 0 0\n",
      " 1 0 2 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 2 0 1 0 0 0 0 0 2 0 0 0 0 0 2 1 0 1 0 0 0 2 1 0 0 0 1 2 0 0 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 4 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 2 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 1 2 0 2 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 2 3 4 0 1 0 0 0\n",
      " 0 2 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 1 2\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 2 0 2 0 0 0 2 2 2 2 0 0 0 0 0 1 1 2 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 2 0 1 0 0 0 0\n",
      " 0 2 0 1 0 0 0 0 1 0 0 0 0 0 0 0 2 0 5 0 0 0 0 2 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 2 0 0 1 5 0 0 0 2 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 2 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 6 1 0 0 0 2 1 2 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0\n",
      " 0 0 2 0 0 1 1 0 0 0 1 1 0 0 2 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 3 0 0\n",
      " 0 0 1 0 0 0 2 0 0 0 1 2 0 0 0 2 0 0 0 0 0 0 1 0 1 2 1 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 1 0 2 1 0 0 1 1 0 0 2 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 1 0 2\n",
      " 0 1 1 0 1 1 0 3 0 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 5 0 0\n",
      " 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# ---------Parch--- TRAIN\n",
    "del values\n",
    "print (df_train['Parch'].unique())\n",
    "values = array(df_train['Parch'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_train['Parch0']=onehot_encoded[:,0]\n",
    "df_train['Parch1']=onehot_encoded[:,1]\n",
    "df_train['Parch2']=onehot_encoded[:,2]\n",
    "df_train['Parch3']=onehot_encoded[:,3]\n",
    "df_train['Parch4']=onehot_encoded[:,4]\n",
    "df_train['Parch5']=onehot_encoded[:,5]\n",
    "df_train['Parch6']=onehot_encoded[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 2 4 6 5 9]\n",
      "[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 3 0 1 0 0 0 0 0 2 2 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 1 2 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 4 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 4 0 0 0 0 0 0 6 2 0 3 0 0 0 0 0\n",
      " 0 1 1 0 0 2 2 0 0 0 0 2 0 1 0 0 0 1 0 2 0 0 0 0 0 0 5 2 0 0 3 2 0 1 0 0 1\n",
      " 0 1 0 2 0 0 0 1 0 2 0 2 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 1 1 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 1 0 2 0 0 1 0 0 2 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 2 0 0 0 0 0 1 0 0 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0\n",
      " 1 0 0 0 2 0 0 0 0 9 1 1 0 0 0 0 0 1 0 0 2 2 0 0 0 0 0 2 1 0 0 0 9 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 2 0 0 0 1 0 1 2 0 1 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 1]\n",
      "[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 3 0 1 0 0 0 0 0 2 2 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 1 2 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 4 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 4 0 0 0 0 0 0 6 2 0 3 0 0 0 0 0\n",
      " 0 1 1 0 0 2 2 0 0 0 0 2 0 1 0 0 0 1 0 2 0 0 0 0 0 0 5 2 0 0 3 2 0 1 0 0 1\n",
      " 0 1 0 2 0 0 0 1 0 2 0 2 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 1 1 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 1 0 2 0 0 1 0 0 2 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 2 0 0 0 0 0 1 0 0 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0\n",
      " 1 0 0 0 2 0 0 0 0 7 1 1 0 0 0 0 0 1 0 0 2 2 0 0 0 0 0 2 1 0 0 0 7 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 2 0 0 0 1 0 1 2 0 1 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# ---------Parch--- TEST\n",
    "# Apareix un 9 que no se que es, aquest el deixarem sense omplir\n",
    "del values\n",
    "print (df_test['Parch'].unique())\n",
    "values = array(df_test['Parch'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "df_test['Parch0']=onehot_encoded[:,0]\n",
    "df_test['Parch1']=onehot_encoded[:,1]\n",
    "df_test['Parch2']=onehot_encoded[:,2]\n",
    "df_test['Parch3']=onehot_encoded[:,3]\n",
    "df_test['Parch4']=onehot_encoded[:,4]\n",
    "df_test['Parch5']=onehot_encoded[:,5]\n",
    "df_test['Parch6']=onehot_encoded[:,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ara afegiria una categoritzacio del FARE (sembla que te certa correlacio amb supervivencia)\n",
    "Al test, el Fare te 1 NaN... podriem veure quina classe es, i li posem la mitja en funcio de la classe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuU3GWd5/H3t6r6klt359KBzo0EEpCIyKUNIDqjIhovM8ERDlFG2Rlms6Oycz8r7I6sMs6OzF6Y8YiuKIyIw4CL49qrcaICo8hgTAcCIQmBJglJJyHpkPulL1X13T9+TzWVSnX6193V3amqz+ucOv27PL9fnqdp+tvP3dwdERGRxHhnQEREzgwKCCIiAiggiIhIoIAgIiKAAoKIiAQKCCIiAiggiIhIoIAgIiKAAoKIiASpOInMbCnw90AS+Ka7f6ngfh3wbeBy4HXgRnfflnd/HrAR+Ly7/4847yxmxowZPn/+/DhZFhGRYO3atfvcvXmwdIMGBDNLAvcA1wKdwBoza3P3jXnJbgEOuPtCM1sO3AXcmHf/buDHQ3znKebPn097e/tgWRYRkTxm9mqcdHGajJYAHe6+xd17gYeBZQVplgEPhONHgWvMzEJGrgO2ABuG+E4RERlDcQLCbGBH3nlnuFY0jbungUPAdDObBHwW+MIw3ikiImMoTkCwItcKl0gdKM0XgLvd/egw3hklNFthZu1m1t7V1TVoZkVEZHjidCp3AnPzzucAuwZI02lmKaAR2A9cAVxvZn8LNAFZM+sG1sZ4JwDufi9wL0Bra6vW6hYRGSVxAsIaYJGZLQB2AsuBjxekaQNuBp4Grgce92ijhXfmEpjZ54Gj7v6VEDQGe6eIiIyhQQOCu6fN7FZgFdEQ0fvdfYOZ3Qm0u3sbcB/woJl1ENUMlg/nnSMsi4iIjICV045pra2trmGnIiJDY2Zr3b11sHSaqSwiIoACQlH//tvt/PdVL453NkRExpQCQhGbdh/msU17xzsbIiJjSgGhiN50lo69R+nuy4x3VkRExowCQhF9mSzprPPSniPjnRURkTGjgFBEbzoLwIZdh8c5JyIiY0cBoYi+TDQUd8OuQ+OcExGRsaOAUMDd6c1ENYQXdqqGICLVQwGhQK52kDB48bXDpENwEBGpdAoIBXK1g/PPmkJ3X5Yt+46Nc45ERMaGAkKBvtChfOm8qYD6EUSkeiggFMjVEC5smUJdKsEG9SOISJWIs/x1VXho9XYADhzrBeC5HQdpnlLHC6ohiEiVUA2hQDobdSonEwlmNU1gw67DlNOKsCIiw6WAUCDTHxCMGZNqOdKd5khPepxzJSIy+hQQCqSzUR9CKmGkktG3R2saiUg1UEAokF9DqAkBoadPcxFEpPIpIBTI9SGkEkZN0gDVEESkOsQKCGa21Mw2m1mHmd1W5H6dmT0S7q82s/nh+hIzWxc+z5nZR/Ke2WZm68O9M2ZfzGI1hG7VEESkCgw67NTMksA9wLVAJ7DGzNrcfWNesluAA+6+0MyWA3cBNwIvAK3unjazFuA5M/t/7p7rpX23u+8rZYFGKtNfQ0iQSkaBoDutGoKIVL44NYQlQIe7b3H3XuBhYFlBmmXAA+H4UeAaMzN3P573y78eOOPHb6bzawgJdSqLSPWIExBmAzvyzjvDtaJpQgA4BEwHMLMrzGwDsB74w7wA4cBPzGytma0YfhFKK5M3ykhNRiJSTeLMVLYi1wr/0h8wjbuvBt5sZhcCD5jZj929G7ja3XeZ2Uzgp2b2orv/4pR/PAoWKwDmzZsXI7sj09+HkDRSrk5lEakecWoIncDcvPM5wK6B0phZCmgE9ucncPdNwDHgonC+K3zdC3yfqGnqFO5+r7u3untrc3NzjOyOTLrYsNO0aggiUvniBIQ1wCIzW2BmtcByoK0gTRtwczi+Hnjc3T08kwIws3OAC4BtZjbJzKaE65OA9xF1QI+7TN6w05SGnYpIFRm0ySiMELoVWAUkgfvdfYOZ3Qm0u3sbcB/woJl1ENUMlofH3wHcZmZ9QBb4tLvvM7Nzge+bWS4PD7n7v5S6cMORzrxRQzAUEESkesRa7dTdVwIrC67dkXfcDdxQ5LkHgQeLXN8CvHWomR0LGX9j2KlbdKwmIxGpBlr+ukA6bwtNzDBTDUFEqoMCQoFM1kkljNCcRX0qqYAgIlVBaxkVyGSzJBNvjKKtr0loHoKIVAUFhALprBcEBNUQRKQ6KCAUyDUZ5dTXJOlWp7KIVAEFhAKFNYS6VIIe1RBEpAooIBTIZJ1k4o1vS51qCCJSJRQQCqQLm4xSCfUhiEhVUEAocOooo6SajESkKiggFDilhqBhpyJSJRQQCmSKDTvVjmkiUgUUEApkst6/yiloprKIVA8FhAKZrJM0NRmJSPVRQCiQzjjJ5BvfFs1UFpFqoYBQIOMndyrXpRL0pLO4F+4aKiJSWRQQCqQzJw87ratJAtoTQUQqnwJCgWKjjAB61I8gIhVOAaFAsXkIgIaeikjFixUQzGypmW02sw4zu63I/TozeyTcX21m88P1JWa2LnyeM7OPxH3neDmlhpCKagjqWBaRSjdoQDCzJHAP8AFgMfAxM1tckOwW4IC7LwTuBu4K118AWt39EmAp8HUzS8V855hz96LLXwMaeioiFS9ODWEJ0OHuW9y9F3gYWFaQZhnwQDh+FLjGzMzdj7t7OlyvB3JDdeK8c8xlPcpg4Y5poBqCiFS+OAFhNrAj77wzXCuaJgSAQ8B0ADO7wsw2AOuBPwz347yT8PwKM2s3s/aurq4Y2R2+TDaKV6nEyfMQQAFBRCpfnIBgRa4VDsofMI27r3b3NwNvA243s/qY7yQ8f6+7t7p7a3Nzc4zsDl8uIBStIWjYqYhUuDgBoROYm3c+B9g1UBozSwGNwP78BO6+CTgGXBTznWMunY1+6Z+8Y5pqCCJSHeIEhDXAIjNbYGa1wHKgrSBNG3BzOL4eeNzdPTyTAjCzc4ALgG0x3znm3mgyOrWGoIlpIlLpUoMlcPe0md0KrAKSwP3uvsHM7gTa3b0NuA940Mw6iGoGy8Pj7wBuM7M+IAt82t33ARR7Z4nLNmTpIk1GqiGISLUYNCAAuPtKYGXBtTvyjruBG4o89yDwYNx3jrf+GkLy1E5l7ZomIpVOM5Xz9NcQrNiwUzUZiUhlU0DI80YNodjENNUQRKSyKSDkKTbKqCaZIJkwrWUkIhVPASFPsVFGAPUp7ZomIpVPASFPJnPqKCOI9kRQk5GIVDoFhDzFhp2CaggiUh0UEPIUW7oCwr7K6kMQkQqngJAnXWRxO4iajLRjmohUOgWEPAPXEBL0qIYgIhVOASFPbtjpqaOM1KksIpVPASHP6WoI6lQWkUqngJBnwHkIGnYqIlVAASFPrlM5oVFGIlKFFBDyZLJO0oyEFUxM0zwEEakCCgh5Mlk/pf8A1GQkItVBASFPOpstGhDqahKahyAiFU8BIU8m66d0KEM07LQ3kyUb+hhERCqRAkKedMZJJos3GYH2VRaRyhYrIJjZUjPbbGYdZnZbkft1ZvZIuL/azOaH69ea2VozWx++vifvmX8N71wXPjNLVajhyriftFtazhu7pqkfQUQq16B7KptZErgHuBboBNaYWZu7b8xLdgtwwN0Xmtly4C7gRmAf8FvuvsvMLgJWAbPznrvJ3dtLVJYRS2f8pN3Scvp3TdPQUxGpYHFqCEuADnff4u69wMPAsoI0y4AHwvGjwDVmZu7+rLvvCtc3APVmVleKjI+GgUcZaV9lEal8cQLCbGBH3nknJ/+Vf1Iad08Dh4DpBWk+Cjzr7j151/4hNBd9zqxIWw1gZivMrN3M2ru6umJkd/iiTuVTvyX1Ke2rLCKVL05AKPaLunC4zWnTmNmbiZqR/kPe/Zvc/S3AO8PnE8X+cXe/191b3b21ubk5RnaHLz1ADaFOfQgiUgXiBIROYG7e+Rxg10BpzCwFNAL7w/kc4PvAJ939ldwD7r4zfD0CPETUNDWuMtnsgMNOQU1GIlLZ4gSENcAiM1tgZrXAcqCtIE0bcHM4vh543N3dzJqAHwG3u/tTucRmljKzGeG4Bvgw8MLIijJyA/Uh1KlTWUSqwKABIfQJ3Eo0QmgT8F1332Bmd5rZb4dk9wHTzawD+DMgNzT1VmAh8LmC4aV1wCozex5YB+wEvlHKgg3HQE1GuU7lHjUZiUgFG3TYKYC7rwRWFly7I++4G7ihyHNfBL44wGsvj5/NsTHgTGVNTBORKqCZynmiGkKRUUY1GmUkIpVPASHPwGsZaR6CiFQ+BYQ8A612OrE2alk73qsagohULgWEPAP3ISRIJowj3X3jkCsRkbGhgJBnoGGnZsaU+hRHutPjkCsRkbGhgBBk3ck6RZe/BkJAUA1BRCqXAkKQCZvfpIovqcSUuhrVEESkoikgBOlMFBCSyeLfEjUZiUilU0AIMh4CQpE+BIAp9TUcVpORiFQwBYQgnYnmGBQbZQTQoBqCiFQ4BYQg14cwcA1BncoiUtkUEIJ0rlP5NE1GR3vSuBduBSEiUhkUEII4NYSswzHNVhaRCqWAEGRi1BAANRuJSMVSQAjS/TWEgYedAupYFpGKpYAQDF5DyAUE1RBEpDIpIAT9w04HXLoiajI6rBqCiFSoWAHBzJaa2WYz6zCz24rcrzOzR8L91WY2P1y/1szWmtn68PU9ec9cHq53mNmXzQZYM2KMvDHKqPi3pEFNRiJS4QYNCGaWBO4BPgAsBj5mZosLkt0CHHD3hcDdwF3h+j7gt9z9LcDNwIN5z3wNWAEsCp+lIyjHiKWzp5+Ypk5lEal0cWoIS4AOd9/i7r3Aw8CygjTLgAfC8aPANWZm7v6su+8K1zcA9aE20QI0uPvTHg3s/zZw3YhLMwK5tYwGbjJSDUFEKlucgDAb2JF33hmuFU3j7mngEDC9IM1HgWfdvSek7xzknWOqv8logMXtJtYmtUmOiFS0VIw0xf5kLpyue9o0ZvZmomak9w3hnblnVxA1LTFv3rzB8jpsA61l9NDq7f3HtckEa189MGp5EBEZT3FqCJ3A3LzzOcCugdKYWQpoBPaH8znA94FPuvsreennDPJOANz9XndvdffW5ubmGNkdnsGWroBoK83uvuyo5UFEZDzFCQhrgEVmtsDMaoHlQFtBmjaiTmOA64HH3d3NrAn4EXC7uz+VS+zuu4EjZnZlGF30SeAHIyzLiKQHWboCoL4mSXeflq4Qkco0aEAIfQK3AquATcB33X2Dmd1pZr8dkt0HTDezDuDPgNzQ1FuBhcDnzGxd+MwM9z4FfBPoAF4BflyqQg1HOpMllTBON/q1LpVUDUFEKlacPgTcfSWwsuDaHXnH3cANRZ77IvDFAd7ZDlw0lMyOpnTWBxxhlFNfk+DQCXUqi0hl0kzlIJ3xAdcxylGTkYhUMgWEIJ11ak7TfwDqVBaRyqaAEKSz2dN2KAPUp5L0pDPaJEdEKpICQpDOODUDTErLqa9JknU4rk1yRKQCKSAE6Wx20E7lupro26XlK0SkEikgBFGn8mB9CElAC9yJSGVSQAiiTuVBmoxSUUDQnggiUokUEIJYncqhyehojwKCiFQeBYQgnYkzMU1NRiJSuRQQgnQ23igjUKeyiFQmBYQgnYkzDyE3ykg1BBGpPAoIQTrrp136GqA2lcBQDUFEKpMCQhCnycjMqKtJKCCISEVSQAjiNBlB1I9wWE1GIlKBFBCATNbJOoOOMoJoLoJqCCJSiRQQgN50bj/lwb8d9TUJdSqLSEWKtUFOpXsjIMRrMtr++nEeWr39lHsfv2JeyfMmIjJWVEMAetLR6qVxmoymT6plz5Eejveq2UhEKkusgGBmS81ss5l1mNltRe7Xmdkj4f5qM5sfrk83syfM7KiZfaXgmX8N7yzca3nM9QyhyejSeVPJZJ3nOw+NdrZERMbUoL8BzSwJ3AN8AFgMfMzMFhckuwU44O4LgbuBu8L1buBzwF8M8Pqb3P2S8Nk7nAKUQn9AiFFDaGms5+yGep7ZfmC0syUiMqbi1BCWAB3uvsXde4GHgWUFaZYBD4TjR4FrzMzc/Zi7/5IoMJyx+puMYvQhmBmXnTOVzgMn2HP4jC6WiMiQxAkIs4Edeeed4VrRNO6eBg4B02O8+x9Cc9HnzGzw38ajZCijjAAumdtEwlAtQUQqSpzfgMV+URduKhwnTaGb3P0twDvD5xNF/3GzFWbWbmbtXV1dg2Z2OIbSZAQwuS7FBWc3sG77QTJZ7a8sIpUhTkDoBObmnc8Bdg2UxsxSQCOw/3Qvdfed4esR4CGipqli6e5191Z3b21ubo6R3aHrGcKw05zL5zVxpCfNy3uPjEqeRETGWpyAsAZYZGYLzKwWWA60FaRpA24Ox9cDj7v7gH86m1nKzGaE4xrgw8ALQ818qfQ3GQ2yllG+88+ewsTaJM+8qmYjEakMg05Mc/e0md0KrAKSwP3uvsHM7gTa3b0NuA940Mw6iGoGy3PPm9k2oAGoNbPrgPcBrwKrQjBIAj8DvlHSkg3BUDqVc1KJBJfObeJXW/dzvCfNxDrN8ROR8hbrt5i7rwRWFly7I++4G7hhgGfnD/Day+NlcfQNZaZyvsvOmcpTr7zOc50Hueq8GaORNRGRMaOZyuR3Kg/t29HSOIGWxnqe2X5wNLIlIjKmFBCAnr6hNxnlXDZvKjsPnuC1Q5qTICLlTQEB6M0MbdhpvkvmNpE0Y90OdS6LSHlTQAB6+oY2MS3fpLoUZzXUsedwT6mzJSIyphQQiGoIBgyjxQiAhgk12kVNRMqeAgJRp3IqaQx39YyGCTUcOqGAICLlTQGBqFN5OM1FOQ31NRzvzdAdOqdFRMqRAgJRk9FwOpRzGidE0zn2qh9BRMqYAgJRp/JwhpzmNEyoAWD3oROlypKIyJhTQAB6MtkRNxkBvKb9EUSkjCkgEGoII2oyigKCNswRkXKmgEC0uN1Imozqa5LUphLs1mxlESljCghEi9sNdR2jQo31NaohiEhZU0AgzEMYQQ0BoGFCSjUEESlrCgiEGsJIA0J9DXsUEESkjCkgEPoQRtpkNKGGPUd6tMeyiJQtBQRK1WRUQybrvH5Uk9NEpDwpIJDrVB5ZQGjsn5ymZiMRKU+xAoKZLTWzzWbWYWa3FblfZ2aPhPurzWx+uD7dzJ4ws6Nm9pWCZy43s/XhmS/bcFeWK4GedJbkCCamgSaniUj5G/S3oJklgXuADwCLgY+Z2eKCZLcAB9x9IXA3cFe43g18DviLIq/+GrACWBQ+S4dTgFLoTWepKcEoI9DkNBEpX3H+LF4CdLj7FnfvBR4GlhWkWQY8EI4fBa4xM3P3Y+7+S6LA0M/MWoAGd3/a3R34NnDdSAoyXO4eOpVHFhAm1aVIJUxNRiJStuIEhNnAjrzzznCtaBp3TwOHgOmDvLNzkHcCYGYrzKzdzNq7urpiZHdo0lkn64y4yShhxlkN9Rp6KiJlK85vwWJ/OheOrYyTZljp3f1ed29199bm5ubTvHJ4etPR9pk1I6whAJzdWK8agoiUrTgBoROYm3c+B9g1UBozSwGNwP5B3jlnkHeOiZ50bj/lEgSEhnr1IYhI2YoTENYAi8xsgZnVAsuBtoI0bcDN4fh64PHQN1CUu+8GjpjZlWF00SeBHww59yXQ2x8QRj4CN1dDOE3RRUTOWKnBErh72sxuBVYBSeB+d99gZncC7e7eBtwHPGhmHUQ1g+W5581sG9AA1JrZdcD73H0j8CngW8AE4MfhM+Z60tG2lyPtVIaohnCiL8PhE2kaJ9aM+H0iImNp0IAA4O4rgZUF1+7IO+4Gbhjg2fkDXG8HLoqb0dGSazJKlqDJaFbTBAB2HTqhgCAiZafqZyq/0ak88m/FrKZ6AHYd1FaaIlJ+qj4g9DcZlbSGoI5lESk/Cgi5JqMS9CE0T66jJmmqIYhIWVJAyDUZlWCUUSIRTU5TQBCRcqSA0Fe6TmWImo12H1STkYiUn6oPCL2ZMA+hBE1GALObJrBTNQQRKUNVHxB6+qJO5VI0GQG0NEazlbVzmoiUm6oPCLkaQik6lSFqMkpnna4j2jlNRMpL1QeEXB9CqWoIs/Mmp4mIlBMFhBLOVAZo0eQ0ESlTVR8Q+he3K2GTESggiEj5qfqA0JPOUJM0EiXa0rmhvobJdSl2aeipiJSZqg8IvekstSVYxyjfrCZNThOR8lP1AaEnnaWuJlnSd85qmqCd00Sk7CggpDMlryG0NE5QDUFEyk7VB4Tuvix1NaX9Nsxuquf1Y710h0lvIiLloOoDwv5jvUyfVFvSd7Y0RiON1GwkIuUkVkAws6VmttnMOszstiL368zskXB/tZnNz7t3e7i+2czen3d9m5mtN7N1ZtZeisIMR9eRHpqn1JX0nRp6KiLlaNCAYGZJ4B7gA8Bi4GNmtrgg2S3AAXdfCNwN3BWeXUy0v/KbgaXAV8P7ct7t7pe4e+uISzJMXUdLHxBys5W1yJ2IlJM4eyovATrcfQuAmT0MLAM25qVZBnw+HD8KfMXMLFx/2N17gK1m1hHe93Rpsj8yfZks+4/10jy5viTve2j1dgAyWacmafzg2Z2kM87Hr5hXkveLiIymOE1Gs4Edeeed4VrRNO6eBg4B0wd51oGfmNlaM1sx9KyP3OtHewFKXkNIJozZTRPYvv94Sd8rIjKa4gSEYlN4C9d2HijN6Z692t0vI2qK+oyZ/UbRf9xshZm1m1l7V1dXjOzGl1uRtNQBAWDetInsOtRNOqymKiJyposTEDqBuXnnc4BdA6UxsxTQCOw/3bPunvu6F/g+UVPSKdz9XndvdffW5ubmGNmNb++RaBTQaASEudMmksm6OpZFpGzECQhrgEVmtsDMaok6idsK0rQBN4fj64HH3d3D9eVhFNICYBHwazObZGZTAMxsEvA+4IWRF2doRrOGMHfaRAC2H1BAEJHyMGinsrunzexWYBWQBO539w1mdifQ7u5twH3Ag6HTeD9R0CCk+y5RB3Qa+Iy7Z8zsLOD7Ub8zKeAhd/+XUSjfaeUCwozJpZ2HANEid00Ta9SPICJlI84oI9x9JbCy4NodecfdwA0DPPvXwF8XXNsCvHWomS21rqM9NE2soS5V2rWMcuZOnXjagJAblZRPI5JEZLxU9UzlriM9NE8ufXNRzrxpEzl0oo/XTjNjOWpZExEZfwoIo9B/kDMv9CM8u/3AKfd60hn++ZlO/udPX+J4T3rU8iAiEld1B4RRmKWcr6WxnmTCeKYgIOw72sPHv7Ga9lcPcOBYLz/ZuGfU8iAiEld1B4RRbjJKJRPMbprAL17aR1+Yj3DoRB83/O+n2bDrEMvfNpe3nzedNdv2s1OjkURknFVtQDjWk+Z4b2ZUawgAV507nc17jvDXP9pENuv8+XefY8f+43z796/g4jlNXHPhWUyqS9H23E6yg/QnHDrexzef3MLN9/+aiz+/ih+s2zmqeReR6hJrlFElGs05CPneOreJSXUp7n9qK1v3HePnL3Vxx4cXs2TBNDr2HqW+JsnSi87m0bWdPLfjIL975TlF3/Otp7bxzV9uofPACZon11GTTPDZ7z3PVedOZ2ZDadZiEpHqVrU1hL1jFBAA/vMH38Tbz5vOz1/q4kMXt/B7V88/6f6lc5toaaznic17yWRPrSVkss5323ew88AJfveKc/jTa8/n5qvmk844f/l/X9BIJREpiaoNCGNVQ4CoL+GrN13GX37oQu766MWECXn9zIx3XzCTfUd7+eHzhauCwN+s3MTG3Yf50MUtLJ7VAMCMKXW898Kz+MnGPfxo/e5RL4OIVL4qDgjR3ICZU8amuaVpYi1/8M5zmVxXvJVu8awGZk6p4yuPd5DNqyU8+PQ2vvnLrVx13nTeft6Mk565euEMLp7TyOfbNnJUQ1dFZISqNyAc7SGVMJom1Ix3VgBIhFrCy3uPsvKF6C/+x1/cw39t28B7L5zJh97ScsozyYRx57KL2He0h3ue6BjrLItIhanegHCkhxmT60gkiq3QPT7eMqeRc2dM4taHnuWyv/opn/rOMyye1cDfL7+UhBXP5yVzm/idS2dz35Nb2f661k0SkeGr6lFGY9F/MBQJM37nsjms7zzI/uN9ZLJZ3rf4bH6w7tR+hZyHVm9n0VlTcHbxqX9cy01XRKOUtCaSiAxV9QaEoz1j1n9QbBG7gUybVMtvXjBzSO9vnFDDb54/k59t2sOm3Ye5sKVhqFkUEanuJqPRnKU81n5j0QxaGuv53jOdHO7uG+/siEgZqsqAkMk6+472nnFNRiORSia4sXUufZks31vbedJIJRGROKoyIPzi5S4yWe8f018pZjbU88G3tPDy3qN848kt450dESkzVdmH8J2nX6U5TOyqNEvmR0ti/O2qzVw6bypLFkw7JY2787Wfv8KXH3uZ7r4sZvAbi5r53IcvZOHMKSXLSybrJM+gUVwicnpVFxB27D/O45v38h/fvZDaVOVVkMyMj142hwd/9Sq3PvQMP/yjd5zUef6tp7bxvWc6Wb/zEG86ewotjRM4r3kSj7Tv4P1/9ySfvOoc/uSa82mcWHx+xtZ9x/je2k5mNU3gI5fOZkLtG7vNPbR6O0d70qzbcZBntx9gz+FuPnnVfP7omkVMm1T6bUrPVPuP9dJQnyKVrLyfL6lsFmcdHDNbCvw90Z7K33T3LxXcrwO+DVwOvA7c6O7bwr3bgVuADPBH7r4qzjuLaW1t9fb29tiFK+ZLP36Rbzy5hV9+9t20NE7ovz6UkUDl4LJzmrjunqd409kN/NWyi3jLnEaefLmLP3l4HfuP9fL+N5/NOxfN6F9G42hPmp9t3MOabfuZUJvkvReexV0fvZgJtUncnac6Xuf+p7byxOa94ODAhJokl8xr4oKzpnBWQz2/2vI6//bKPvoyzpypE5gxuY71Ow8xsSbJp9+9kN+7ej71NaOzXelAuo70YAYzBhhAkMk6h070caIvw6zG+lOWFYnrodXb2XXwBI+/uJeNuw8zdWIN71zUzN9ef/GYl1mkkJmtdffWQdMNFhDMLAm8BFwLdAJrgI+5+8a8NJ8GLnb3PzSz5cBH3P1GM1sM/BOwBJgF/Aw4PzyBUsVcAAAJuElEQVR22ncWM9KA0N2X4aq/eYwlC6bx9U+c/L2ptIDw8SvmsXL9bj776PMc6UmzcOZkOvYeZfqkWq67dDbnNU8u+tyugyf40frdbN13jNpUgisWTGPP4W5e2nOUGZNruemKc5hYm+T1o73825bX2bT7cP+CfAZcPKeRd10wk7PCCqxLFkzlSz9+kZ9t2susxnp+/x0LmD99Emc31rPorMmjsp/1A/+2jfU7D/Hs9gNs6TqGA82T67jmwpnMnzGJlsZ6Nu46zM9f6mLzniPk/heY1VTP28+bwcWzG0klE7Hncjy34yCf/d7zvPjaEeprErSeM41XXz/GjgMnaGms56+WXcR7F1de86SM3N7D3bzSdYwdB45Tl0rwm+c30zSx9LXpUgaEq4DPu/v7w/ntAO7+N3lpVoU0T5tZCngNaAZuy0+bSxceO+07ixluQDjem+Znm/by6NpOfvFSF9+55QresejkdYEqMSAAHOnu46HV2/nh87u55sKZTJ1YS80gTRnuzitdxzCDX7zUxcTaJJ+4aj6/9dYW6lLJk75Xveksr75+jJ0HT/Cmsxs4u/HkuR25fDz9yuv8t5WbWL/zUP+92mSCxbMauHReE5fOm8pFsxqYPrmOKXWpIc0gz2adrqM97Nh/nMde3Mu3ntrGib4M0ybVcsncJmqTCbbuO8beI90cOB4Nya1JGm+bP43Lz5nK1n3HyDqs2bafriM91NckWNzSyM1vP4esO0e607yy9ygbdx+m60gPCTPMoua5bNbZsu8YE2qSXL1wBledO72/VrVl3zF++fI+Nu85wtI3n80HL27hkjlNnN1YTyphZ9QseRld2azTl82y+2A39/1yKzv2H+fF147w2uGT91tPJozL503lynOn0Tp/GgtnTmb65NoR/+FUyoBwPbDU3f8gnH8CuMLdb81L80JI0xnOXwGuIPrl/yt3/064fh/w4/DYad9ZzHACgrvzjrueYOfBE8ycUsfyt83lT957/in/M1ZaQCiFgf5CHsr3Kv8d7s5rh7vZc7iHh3+9nZ0HT7Bj/3F2HjxBX+aNn0MzqEmEoGVRzSN3PboU/UJ2j5p8+rLZ/r/yEwZvOruBq86bzrkzJp3SBNTdl+Hg8T6mTqyhrqApJxcI1+04wIZdh+lJZ/vvTapN8qaWhv6d7zw0m7k7c6ZO5MoF0055H8D1l8/hG09u4Z4nOjjemznpnhmkEkYyYQMuTSLlyx3S2SzprFP4azZhcM70SVxw1hRmNU3g3719PvuP9/LYpj08sXkvG3cdJn/k+JT6FE/ffs2Ai2MOJm5AiPP2Yj+phVFkoDQDXS/2J2rRyGRmK4AV4fSomW0eIJ+DepWoberPi9+eAewb7rvLwJDLd1MJ/tFSvCOm/vJt5Y2/OkrpdO2Z9w5wvcTl189oeTupfFuBfw3H/zXGw1O+MKJ/u/jOWwXiBIROYG7e+RygcHGdXJrO0GTUCOwf5NnB3gmAu9/LwP+/lYyZtceJoOVK5St/lV5GlW/8xRkXtwZYZGYLzKwWWA60FaRpA24Ox9cDj3vUFtUGLDezOjNbACwCfh3znSIiMoYGrSG4e9rMbgVWEQ0Rvd/dN5jZnUC7u7cB9wEPmlkHUc1geXh2g5l9l6i2nQY+4+4ZgGLvLH3xREQkrljzEKqBma0IzVMVSeUrf5VeRpVv/CkgiIgIUKWL24mIyKmqPiCY2VIz22xmHWZ223jnZ7jM7H4z2xvmhOSuTTOzn5rZy+Hr1HDdzOzLoczPm9ll45fzeMxsrpk9YWabzGyDmf1xuF4RZTSzejP7tZk9F8r3hXB9gZmtDuV7JAzCIAzUeCSUb7WZzR/P/MdlZkkze9bMfhjOK61828xsvZmtM7P2cK1sfkarOiBYtCzHPcAHgMXAxyxabqMcfQtYWnDtNuAxd18EPBbOISrvovBZAXxtjPI4Emngz939QuBK4DPhv1WllLEHeI+7vxW4BFhqZlcCdwF3h/IdIFoXjPD1gLsvBO4O6crBHwOb8s4rrXwA73b3S/KGmJbPz6i7V+0HuApYlXd+O3D7eOdrBOWZD7yQd74ZaAnHLcDmcPx1orWjTklXLh/gB0RrYVVcGYGJwDNEs/33Aalwvf/nlWiE3lXhOBXS2XjnfZByzSH6hfge4IdEE1crpnwhr9uAGQXXyuZntKprCMBsYEfeeWe4VinOcvfdAOFrbrPmsi53aD64FFhNBZUxNKesA/YCPwVeAQ66ezokyS9Df/nC/UPA9LHN8ZD9HfCfgNyaINOprPJBtOLCT8xsbVhlAcroZ7Tq9kMoEGdZjkpUtuU2s8nA94A/cffDhWsV5Sctcu2MLqNHc3QuMbMm4PvAhcWSha9lVT4z+zCw193Xmtm7cpeLJC3L8uW52t13mdlM4Kdm9uJp0p5xZaz2GkKcZTnK2R4zawEIX/eG62VZbjOrIQoG/+ju/xwuV1QZAdz9INEyN1cCTRYtBwMnl6G/fHbycjFnqquB3zazbcDDRM1Gf0fllA8Ad98Vvu4lCupLKKOf0WoPCJW+hEb+kiI3E7W7565/MoxyuBI4lKvSnqksqgrcB2xy9/+Vd6siymhmzaFmgJlNAN5L1Pn6BNFyMHBq+YotF3NGcvfb3X2Ou88n+v/scXe/iQopH4CZTTKzKblj4H3AC5TTz+h4d8KM9wf4INFmPa8A/2W88zOCcvwTsBvoI/rL4xaiNtfHgJfD12khrRGNrnoFWA+0jnf+Y5TvHUTV6eeBdeHzwUopI3Ax8Gwo3wvAHeH6uUTrf3UA/weoC9frw3lHuH/ueJdhCGV9F/DDSitfKMtz4bMh9/uknH5GNVNZREQANRmJiEiggCAiIoACgoiIBAoIIiICKCCIiEhQ7TOVRU7LzDJEQwJzrnP3beOUHZFRpWGnIqdhZkfdffIwnkt62C5WpFyoyUhkiMxsvpk9aWbPhM/bw/V3hT0bHiLUKszsd8M+B+vM7OthyXWRM5KajEROb0JYgRRgq7t/hGgtmmvdvdvMFhHNEs+tfb8EuMjdt5rZhcCNRAue9ZnZV4GbgG+PcRlEYlFAEDm9E+5+ScG1GuArZnYJkAHOz7v3a3ffGo6vAS4H1oRVWSfwxsJmImccBQSRoftTYA/wVqJm1+68e8fyjg14wN1vH8O8iQyb+hBEhq4R2O3uWeATwED9Ao8B14e18XN7654zRnkUGTIFBJGh+ypws5n9iqi56FixRO6+EfhLoh20nifaBa1lzHIpMkQadioiIoBqCCIiEiggiIgIoIAgIiKBAoKIiAAKCCIiEiggiIgIoIAgIiKBAoKIiADw/wHX+YpybBp4iAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df_train['Fare'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.6271884892086\n",
      "Pclass\n",
      "1    94.280297\n",
      "2    22.202104\n",
      "3    12.459678\n",
      "Name: Fare, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print (df_test['Fare'].mean())\n",
    "print (df_test.groupby(['Pclass'])['Fare'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Pclass                Name   Sex   Age  SibSp  Parch Ticket  \\\n",
      "152         1044       3  Storey, Mr. Thomas  male  60.5      0      0   3701   \n",
      "\n",
      "     Fare Cabin Embarked Deck  Number  Deck_A  Deck_B  Deck_C  Deck_D  Deck_E  \\\n",
      "152   NaN   NaN        S    X     NaN     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "     Deck_F  Deck_G  Deck_X AgeGroup  adult  bebe  infant  jove  nen  vell  \\\n",
      "152     0.0     0.0     1.0     vell    0.0   0.0     0.0   0.0  0.0   1.0   \n",
      "\n",
      "     female  male  Pclass1  Pclass2  Pclass3  SibSp0  SibSp1  SibSp2  SibSp3  \\\n",
      "152     0.0   1.0      0.0      0.0      1.0     1.0     0.0     0.0     0.0   \n",
      "\n",
      "     SibSp4  SibSp5  SibSp8  Parch0  Parch1  Parch2  Parch3  Parch4  Parch5  \\\n",
      "152     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "     Parch6  \n",
      "152     0.0  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 47)\n",
    "print(df_test[df_test['Fare'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El que tenim sense dades es de 3 classe, posem el valor de fare mig per pclass==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_test['Fare'][df_test['Fare'].isnull()] = 12.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Fare FareGroup\n",
      "0      7.2500        20\n",
      "1     71.2833        80\n",
      "2      7.9250        20\n",
      "3     53.1000        60\n",
      "4      8.0500        20\n",
      "5      8.4583        20\n",
      "6     51.8625        60\n",
      "7     21.0750        40\n",
      "8     11.1333        20\n",
      "9     30.0708        40\n",
      "10    16.7000        20\n",
      "11    26.5500        40\n",
      "12     8.0500        20\n",
      "13    31.2750        40\n",
      "14     7.8542        20\n",
      "15    16.0000        20\n",
      "16    29.1250        40\n",
      "17    13.0000        20\n",
      "18    18.0000        20\n",
      "19     7.2250        20\n",
      "20    26.0000        40\n",
      "21    13.0000        20\n",
      "22     8.0292        20\n",
      "23    35.5000        40\n",
      "24    21.0750        40\n",
      "25    31.3875        40\n",
      "26     7.2250        20\n",
      "27   263.0000       300\n",
      "28     7.8792        20\n",
      "29     7.8958        20\n",
      "..        ...       ...\n",
      "861   11.5000        20\n",
      "862   25.9292        40\n",
      "863   69.5500        80\n",
      "864   13.0000        20\n",
      "865   13.0000        20\n",
      "866   13.8583        20\n",
      "867   50.4958        60\n",
      "868    9.5000        20\n",
      "869   11.1333        20\n",
      "870    7.8958        20\n",
      "871   52.5542        60\n",
      "872    5.0000        20\n",
      "873    9.0000        20\n",
      "874   24.0000        40\n",
      "875    7.2250        20\n",
      "876    9.8458        20\n",
      "877    7.8958        20\n",
      "878    7.8958        20\n",
      "879   83.1583       100\n",
      "880   26.0000        40\n",
      "881    7.8958        20\n",
      "882   10.5167        20\n",
      "883   10.5000        20\n",
      "884    7.0500        20\n",
      "885   29.1250        40\n",
      "886   13.0000        20\n",
      "887   30.0000        40\n",
      "888   23.4500        40\n",
      "889   30.0000        40\n",
      "890    7.7500        20\n",
      "\n",
      "[891 rows x 2 columns]\n",
      "[20, 80, 60, 40, 300, 200, 100, 600]\n",
      "Categories (8, object): [20 < 40 < 60 < 80 < 100 < 200 < 300 < 600]\n",
      "['20' '80' '20' '60' '20' '20' '60' '40' '20' '40' '20' '40' '20' '40'\n",
      " '20' '20' '40' '20' '20' '20' '40' '20' '20' '40' '40' '40' '20' '300'\n",
      " '20' '20' '40' '200' '20' '20' '100' '60' '20' '20' '20' '20' '20' '40'\n",
      " '20' '60' '20' '20' '20' '20' '40' '20' '40' '20' '80' '40' '80' '40'\n",
      " '20' '20' '40' '60' '20' '100' '100' '40' '40' '20' '20' '20' '20' '20'\n",
      " '20' '60' '80' '20' '60' '20' '20' '20' '40' '20' '20' '20' '20' '60'\n",
      " '20' '20' '40' '20' '300' '20' '20' '20' '80' '40' '20' '20' '40' '80'\n",
      " '40' '40' '20' '20' '80' '20' '20' '20' '20' '20' '20' '40' '60' '20'\n",
      " '20' '20' '20' '20' '20' '40' '300' '40' '80' '20' '40' '20' '80' '20'\n",
      " '20' '20' '40' '20' '20' '20' '20' '40' '20' '20' '40' '60' '20' '80'\n",
      " '20' '20' '20' '20' '20' '40' '20' '40' '40' '20' '20' '80' '20' '20'\n",
      " '20' '80' '20' '20' '20' '80' '20' '20' '20' '20' '40' '40' '60' '40'\n",
      " '40' '60' '40' '40' '20' '20' '40' '20' '40' '40' '20' '20' '80' '20'\n",
      " '40' '40' '40' '60' '20' '40' '20' '20' '20' '20' '20' '40' '40' '200'\n",
      " '20' '20' '20' '20' '20' '80' '20' '20' '20' '20' '20' '20' '20' '40'\n",
      " '20' '40' '20' '20' '20' '200' '20' '40' '80' '20' '20' '20' '20' '20'\n",
      " '100' '20' '20' '20' '20' '40' '100' '20' '20' '40' '20' '20' '40' '40'\n",
      " '20' '20' '20' '20' '20' '20' '20' '100' '20' '20' '60' '40' '20' '20'\n",
      " '40' '20' '40' '20' '80' '100' '600' '40' '20' '40' '80' '20' '20' '20'\n",
      " '40' '20' '200' '200' '40' '20' '20' '40' '20' '80' '20' '20' '40' '40'\n",
      " '20' '20' '20' '20' '40' '20' '20' '20' '20' '20' '80' '100' '20' '20'\n",
      " '20' '40' '20' '200' '40' '300' '20' '40' '20' '20' '20' '200' '200'\n",
      " '200' '40' '60' '100' '300' '40' '20' '40' '20' '40' '20' '200' '200'\n",
      " '20' '20' '20' '40' '80' '200' '20' '20' '40' '60' '40' '40' '200' '20'\n",
      " '200' '20' '80' '200' '20' '40' '40' '300' '20' '20' '20' '20' '20' '20'\n",
      " '20' '20' '20' '40' '20' '20' '20' '20' '60' '20' '20' '20' '40' '40'\n",
      " '20' '20' '20' '20' '80' '20' '20' '80' '60' '20' '20' '200' '40' '100'\n",
      " '20' '300' '20' '20' '300' '20' '20' '60' '20' '80' '60' '20' '20' '20'\n",
      " '200' '20' '20' '200' '20' '20' '20' '40' '20' '20' '20' '20' '20' '20'\n",
      " '20' '40' '20' '20' '20' '40' '20' '20' '100' '20' '20' '20' '40' '20'\n",
      " '20' '40' '20' '20' '20' '20' '40' '20' '40' '40' '20' '20' '40' '20'\n",
      " '40' '20' '60' '200' '40' '20' '300' '20' '40' '20' '20' '20' '20' '100'\n",
      " '20' '40' '20' '40' '40' '20' '40' '100' '20' '20' '40' '60' '20' '20'\n",
      " '40' '20' '40' '20' '20' '20' '20' '40' '20' '20' '20' '20' '40' '20'\n",
      " '20' '60' '40' '20' '20' '20' '60' '20' '20' '20' '100' '40' '100' '40'\n",
      " '20' '20' '20' '20' '40' '60' '20' '20' '80' '20' '200' '20' '20' '20'\n",
      " '20' '20' '100' '200' '40' '40' '40' '60' '20' '20' '40' '60' '20' '40'\n",
      " '20' '40' '40' '20' '100' '20' '20' '60' '20' '20' '20' '300' '20' '20'\n",
      " '40' '20' '20' '40' '20' '40' '40' '200' '20' '60' '80' '40' '40' '40'\n",
      " '200' '40' '40' '20' '40' '40' '200' '40' '20' '20' '20' '40' '40' '300'\n",
      " '80' '20' '20' '20' '20' '20' '20' '40' '20' '40' '20' '20' '20' '60'\n",
      " '40' '20' '20' '20' '20' '60' '20' '20' '40' '200' '40' '60' '20' '80'\n",
      " '20' '80' '20' '20' '20' '80' '20' '20' '40' '40' '40' '20' '20' '60'\n",
      " '40' '20' '60' '20' '40' '20' '20' '40' '60' '200' '40' '20' '20' '20'\n",
      " '20' '80' '20' '20' '40' '20' '20' '60' '20' '20' '20' '40' '20' '80'\n",
      " '20' '20' '40' '20' '40' '20' '40' '20' '20' '40' '40' '20' '20' '80'\n",
      " '40' '60' '20' '80' '20' '40' '20' '20' '20' '40' '20' '20' '20' '80'\n",
      " '20' '20' '20' '200' '200' '20' '40' '20' '20' '80' '20' '20' '20' '60'\n",
      " '40' '60' '20' '20' '20' '20' '20' '20' '60' '600' '20' '80' '20' '60'\n",
      " '40' '60' '40' '20' '20' '300' '60' '20' '60' '20' '40' '20' '20' '20'\n",
      " '200' '20' '300' '40' '20' '20' '20' '40' '20' '40' '200' '20' '60' '40'\n",
      " '60' '20' '20' '20' '300' '20' '20' '20' '40' '20' '20' '20' '60' '20'\n",
      " '40' '20' '40' '20' '300' '20' '20' '20' '20' '20' '40' '600' '20' '20'\n",
      " '40' '80' '300' '20' '20' '80' '40' '20' '60' '20' '40' '20' '20' '20'\n",
      " '80' '20' '20' '20' '20' '100' '20' '20' '20' '200' '20' '80' '40' '20'\n",
      " '40' '20' '20' '20' '20' '20' '40' '20' '20' '20' '20' '300' '20' '60'\n",
      " '40' '40' '20' '20' '20' '40' '40' '80' '20' '40' '80' '40' '20' '20'\n",
      " '40' '20' '20' '40' '20' '40' '200' '20' '20' '20' '20' '20' '20' '60'\n",
      " '20' '40' '20' '40' '20' '20' '20' '40' '20' '40' '100' '20' '20' '20'\n",
      " '40' '20' '60' '40' '20' '100' '20' '20' '20' '20' '20' '100' '20' '20'\n",
      " '60' '40' '20' '20' '40' '20' '20' '20' '80' '20' '40' '100' '40' '20'\n",
      " '20' '40' '40' '20' '200' '40' '20' '20' '20' '20' '40' '80' '20' '20'\n",
      " '20' '60' '20' '20' '20' '60' '20' '20' '40' '20' '20' '20' '20' '100'\n",
      " '40' '20' '20' '20' '20' '40' '20' '40' '40' '40' '20']\n",
      "[1 7 1 5 1 1 5 4 1 4 1 4 1 4 1 1 4 1 1 1 4 1 1 4 4 4 1 3 1 1 4 2 1 1 0 5 1\n",
      " 1 1 1 1 4 1 5 1 1 1 1 4 1 4 1 7 4 7 4 1 1 4 5 1 0 0 4 4 1 1 1 1 1 1 5 7 1\n",
      " 5 1 1 1 4 1 1 1 1 5 1 1 4 1 3 1 1 1 7 4 1 1 4 7 4 4 1 1 7 1 1 1 1 1 1 4 5\n",
      " 1 1 1 1 1 1 4 3 4 7 1 4 1 7 1 1 1 4 1 1 1 1 4 1 1 4 5 1 7 1 1 1 1 1 4 1 4\n",
      " 4 1 1 7 1 1 1 7 1 1 1 7 1 1 1 1 4 4 5 4 4 5 4 4 1 1 4 1 4 4 1 1 7 1 4 4 4\n",
      " 5 1 4 1 1 1 1 1 4 4 2 1 1 1 1 1 7 1 1 1 1 1 1 1 4 1 4 1 1 1 2 1 4 7 1 1 1\n",
      " 1 1 0 1 1 1 1 4 0 1 1 4 1 1 4 4 1 1 1 1 1 1 1 0 1 1 5 4 1 1 4 1 4 1 7 0 6\n",
      " 4 1 4 7 1 1 1 4 1 2 2 4 1 1 4 1 7 1 1 4 4 1 1 1 1 4 1 1 1 1 1 7 0 1 1 1 4\n",
      " 1 2 4 3 1 4 1 1 1 2 2 2 4 5 0 3 4 1 4 1 4 1 2 2 1 1 1 4 7 2 1 1 4 5 4 4 2\n",
      " 1 2 1 7 2 1 4 4 3 1 1 1 1 1 1 1 1 1 4 1 1 1 1 5 1 1 1 4 4 1 1 1 1 7 1 1 7\n",
      " 5 1 1 2 4 0 1 3 1 1 3 1 1 5 1 7 5 1 1 1 2 1 1 2 1 1 1 4 1 1 1 1 1 1 1 4 1\n",
      " 1 1 4 1 1 0 1 1 1 4 1 1 4 1 1 1 1 4 1 4 4 1 1 4 1 4 1 5 2 4 1 3 1 4 1 1 1\n",
      " 1 0 1 4 1 4 4 1 4 0 1 1 4 5 1 1 4 1 4 1 1 1 1 4 1 1 1 1 4 1 1 5 4 1 1 1 5\n",
      " 1 1 1 0 4 0 4 1 1 1 1 4 5 1 1 7 1 2 1 1 1 1 1 0 2 4 4 4 5 1 1 4 5 1 4 1 4\n",
      " 4 1 0 1 1 5 1 1 1 3 1 1 4 1 1 4 1 4 4 2 1 5 7 4 4 4 2 4 4 1 4 4 2 4 1 1 1\n",
      " 4 4 3 7 1 1 1 1 1 1 4 1 4 1 1 1 5 4 1 1 1 1 5 1 1 4 2 4 5 1 7 1 7 1 1 1 7\n",
      " 1 1 4 4 4 1 1 5 4 1 5 1 4 1 1 4 5 2 4 1 1 1 1 7 1 1 4 1 1 5 1 1 1 4 1 7 1\n",
      " 1 4 1 4 1 4 1 1 4 4 1 1 7 4 5 1 7 1 4 1 1 1 4 1 1 1 7 1 1 1 2 2 1 4 1 1 7\n",
      " 1 1 1 5 4 5 1 1 1 1 1 1 5 6 1 7 1 5 4 5 4 1 1 3 5 1 5 1 4 1 1 1 2 1 3 4 1\n",
      " 1 1 4 1 4 2 1 5 4 5 1 1 1 3 1 1 1 4 1 1 1 5 1 4 1 4 1 3 1 1 1 1 1 4 6 1 1\n",
      " 4 7 3 1 1 7 4 1 5 1 4 1 1 1 7 1 1 1 1 0 1 1 1 2 1 7 4 1 4 1 1 1 1 1 4 1 1\n",
      " 1 1 3 1 5 4 4 1 1 1 4 4 7 1 4 7 4 1 1 4 1 1 4 1 4 2 1 1 1 1 1 1 5 1 4 1 4\n",
      " 1 1 1 4 1 4 0 1 1 1 4 1 5 4 1 0 1 1 1 1 1 0 1 1 5 4 1 1 4 1 1 1 7 1 4 0 4\n",
      " 1 1 4 4 1 2 4 1 1 1 1 4 7 1 1 1 5 1 1 1 5 1 1 4 1 1 1 1 0 4 1 1 1 1 4 1 4\n",
      " 4 4 1]\n"
     ]
    }
   ],
   "source": [
    "# fem grups de fare en plan: 0,20,40,60,80,100,200,300,400,500,600\n",
    "# en aquest bloc definim una columna categoritzant les edats\n",
    "bins= [0,20,40,60,80,100,200,300,400,500,600]\n",
    "labels = ['20','40','60','80','100','200','300','400','500','600']\n",
    "df_train['FareGroup'] = pd.cut(df_train['Fare'], bins=bins, labels=labels, right=False)\n",
    "print(df_train[['Fare','FareGroup']])\n",
    "\n",
    "del values\n",
    "print (df_train['FareGroup'].unique())\n",
    "values = array(df_train['FareGroup'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "\n",
    "# ha codificat aixi: 100-0, 20-1, 200-2, 300-3,  40-4, 60-5, 600-6, 80-7\n",
    "df_train['Fare20']=onehot_encoded[:,0]\n",
    "df_train['Fare40']=onehot_encoded[:,1]\n",
    "df_train['Fare60']=onehot_encoded[:,2]\n",
    "df_train['Fare80']=onehot_encoded[:,3]\n",
    "df_train['Fare100']=onehot_encoded[:,4]\n",
    "df_train['Fare200']=onehot_encoded[:,5]\n",
    "df_train['Fare300']=onehot_encoded[:,6]\n",
    "df_train['Fare600']=onehot_encoded[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Fare FareGroup\n",
      "0      7.8292        20\n",
      "1      7.0000        20\n",
      "2      9.6875        20\n",
      "3      8.6625        20\n",
      "4     12.2875        20\n",
      "5      9.2250        20\n",
      "6      7.6292        20\n",
      "7     29.0000        40\n",
      "8      7.2292        20\n",
      "9     24.1500        40\n",
      "10     7.8958        20\n",
      "11    26.0000        40\n",
      "12    82.2667       100\n",
      "13    26.0000        40\n",
      "14    61.1750        80\n",
      "15    27.7208        40\n",
      "16    12.3500        20\n",
      "17     7.2250        20\n",
      "18     7.9250        20\n",
      "19     7.2250        20\n",
      "20    59.4000        60\n",
      "21     3.1708        20\n",
      "22    31.6833        40\n",
      "23    61.3792        80\n",
      "24   262.3750       300\n",
      "25    14.5000        20\n",
      "26    61.9792        80\n",
      "27     7.2250        20\n",
      "28    30.5000        40\n",
      "29    21.6792        40\n",
      "..        ...       ...\n",
      "388    7.7500        20\n",
      "389   21.0750        40\n",
      "390   93.5000       100\n",
      "391   39.4000        40\n",
      "392   20.2500        40\n",
      "393   10.5000        20\n",
      "394   22.0250        40\n",
      "395   60.0000        80\n",
      "396    7.2500        20\n",
      "397   79.2000        80\n",
      "398    7.7750        20\n",
      "399    7.7333        20\n",
      "400  164.8667       200\n",
      "401   21.0000        40\n",
      "402   59.4000        60\n",
      "403   47.1000        60\n",
      "404   27.7208        40\n",
      "405   13.8625        20\n",
      "406   10.5000        20\n",
      "407  211.5000       300\n",
      "408    7.7208        20\n",
      "409   13.7750        20\n",
      "410    7.7500        20\n",
      "411   90.0000       100\n",
      "412    7.7750        20\n",
      "413    8.0500        20\n",
      "414  108.9000       200\n",
      "415    7.2500        20\n",
      "416    8.0500        20\n",
      "417   22.3583        40\n",
      "\n",
      "[418 rows x 2 columns]\n",
      "[20, 40, 100, 80, 60, 300, 200, 600]\n",
      "Categories (8, object): [20 < 40 < 60 < 80 < 100 < 200 < 300 < 600]\n",
      "['20' '20' '20' '20' '20' '20' '20' '40' '20' '40' '20' '40' '100' '40'\n",
      " '80' '40' '20' '20' '20' '20' '60' '20' '40' '80' '300' '20' '80' '20'\n",
      " '40' '40' '40' '40' '40' '40' '60' '20' '20' '20' '20' '60' '20' '40'\n",
      " '20' '20' '60' '20' '40' '20' '80' '20' '80' '20' '40' '300' '20' '40'\n",
      " '20' '20' '20' '300' '20' '20' '20' '20' '300' '40' '20' '60' '40' '300'\n",
      " '20' '20' '20' '40' '300' '300' '20' '40' '20' '20' '20' '300' '40' '20'\n",
      " '20' '20' '20' '20' '20' '40' '20' '20' '60' '20' '40' '20' '80' '20'\n",
      " '20' '20' '60' '40' '20' '20' '20' '40' '20' '20' '20' '20' '20' '20'\n",
      " '40' '20' '300' '20' '20' '20' '80' '40' '20' '20' '60' '20' '20' '20'\n",
      " '20' '40' '20' '20' '20' '40' '40' '20' '20' '20' '20' '20' '20' '60'\n",
      " '60' '200' '300' '40' '40' '20' '60' '20' '40' '40' '100' '20' '20' '20'\n",
      " '40' '20' '300' '20' '40' '20' '20' '20' '20' '20' '20' '40' '60' '40'\n",
      " '40' '20' '20' '20' '20' '20' '40' '40' '40' '60' '40' '100' '20' '100'\n",
      " '60' '20' '300' '20' '40' '20' '80' '20' '40' '40' '20' '20' '40' '20'\n",
      " '200' '20' '20' '20' '20' '20' '300' '40' '20' '40' '20' '20' '40' '20'\n",
      " '40' '20' '80' '40' '20' '60' '20' '200' '300' '20' '20' '20' '20' '20'\n",
      " '40' '20' '20' '20' '20' '20' '80' '40' '20' '20' '80' '20' '80' '20'\n",
      " '20' '200' '40' '40' '200' '20' '40' '60' '40' '40' '40' '20' '40' '20'\n",
      " '200' '20' '20' '20' '20' '20' '20' '20' '20' '20' '40' '20' '20' '20'\n",
      " '20' '20' '20' '20' '80' '20' '200' '20' '20' '40' '20' '40' '40' '20'\n",
      " '20' '20' '20' '20' '40' '20' '20' '100' '20' '20' '40' '20' '20' '100'\n",
      " '20' '20' '60' '40' '60' '20' '20' '20' '40' '20' '20' '40' '200' '20'\n",
      " '100' '20' '20' '20' '20' '20' '200' '20' '200' '20' '20' '40' '20' '20'\n",
      " '20' '40' '300' '20' '40' '80' '40' '20' '40' '40' '20' '20' '20' '40'\n",
      " '20' '20' '40' '20' '20' '20' '80' '600' '20' '20' '20' '20' '20' '40'\n",
      " '80' '20' '80' '80' '40' '40' '60' '20' '20' '20' '80' '40' '40' '20'\n",
      " '60' '80' '20' '40' '60' '20' '20' '200' '20' '20' '100' '300' '20' '20'\n",
      " '60' '40' '20' '20' '20' '20' '20' '80' '20' '20' '20' '40' '100' '40'\n",
      " '40' '20' '40' '80' '20' '80' '20' '20' '200' '40' '60' '60' '40' '20'\n",
      " '20' '300' '20' '20' '20' '100' '20' '20' '200' '20' '20' '40']\n",
      "[1 1 1 1 1 1 1 4 1 4 1 4 0 4 7 4 1 1 1 1 5 1 4 7 3 1 7 1 4 4 4 4 4 4 5 1 1\n",
      " 1 1 5 1 4 1 1 5 1 4 1 7 1 7 1 4 3 1 4 1 1 1 3 1 1 1 1 3 4 1 5 4 3 1 1 1 4\n",
      " 3 3 1 4 1 1 1 3 4 1 1 1 1 1 1 4 1 1 5 1 4 1 7 1 1 1 5 4 1 1 1 4 1 1 1 1 1\n",
      " 1 4 1 3 1 1 1 7 4 1 1 5 1 1 1 1 4 1 1 1 4 4 1 1 1 1 1 1 5 5 2 3 4 4 1 5 1\n",
      " 4 4 0 1 1 1 4 1 3 1 4 1 1 1 1 1 1 4 5 4 4 1 1 1 1 1 4 4 4 5 4 0 1 0 5 1 3\n",
      " 1 4 1 7 1 4 4 1 1 4 1 2 1 1 1 1 1 3 4 1 4 1 1 4 1 4 1 7 4 1 5 1 2 3 1 1 1\n",
      " 1 1 4 1 1 1 1 1 7 4 1 1 7 1 7 1 1 2 4 4 2 1 4 5 4 4 4 1 4 1 2 1 1 1 1 1 1\n",
      " 1 1 1 4 1 1 1 1 1 1 1 7 1 2 1 1 4 1 4 4 1 1 1 1 1 4 1 1 0 1 1 4 1 1 0 1 1\n",
      " 5 4 5 1 1 1 4 1 1 4 2 1 0 1 1 1 1 1 2 1 2 1 1 4 1 1 1 4 3 1 4 7 4 1 4 4 1\n",
      " 1 1 4 1 1 4 1 1 1 7 6 1 1 1 1 1 4 7 1 7 7 4 4 5 1 1 1 7 4 4 1 5 7 1 4 5 1\n",
      " 1 2 1 1 0 3 1 1 5 4 1 1 1 1 1 7 1 1 1 4 0 4 4 1 4 7 1 7 1 1 2 4 5 5 4 1 1\n",
      " 3 1 1 1 0 1 1 2 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "df_test['FareGroup'] = pd.cut(df_test['Fare'], bins=bins, labels=labels, right=False)\n",
    "print(df_test[['Fare','FareGroup']])\n",
    "\n",
    "del values\n",
    "print (df_test['FareGroup'].unique())\n",
    "values = array(df_test['FareGroup'])\n",
    "print(values)\n",
    "onehot_encoded = hot_encode_PC(values)\n",
    "\n",
    "\n",
    "# ha codificat aixi: 100-0, 20-1, 200-2, 300-3,  40-4, 60-5, 600-6, 80-7\n",
    "df_test['Fare20']=onehot_encoded[:,0]\n",
    "df_test['Fare40']=onehot_encoded[:,1]\n",
    "df_test['Fare60']=onehot_encoded[:,2]\n",
    "df_test['Fare80']=onehot_encoded[:,3]\n",
    "df_test['Fare100']=onehot_encoded[:,4]\n",
    "df_test['Fare200']=onehot_encoded[:,5]\n",
    "df_test['Fare300']=onehot_encoded[:,6]\n",
    "df_test['Fare600']=onehot_encoded[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# el dataframe amb el que aplicarem el model: df_Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ara ens quedem amb un dataframe amb nomes aquells labels que ens interssen pel model\n",
    "df_Titanic = df_train[['Survived','adult','bebe','infant','jove','nen','vell','female','male','Pclass1','Pclass2',\n",
    "                       'Pclass3','Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_F','Deck_G','Deck_X',\n",
    "                       'SibSp0','SibSp1','SibSp2','SibSp3','SibSp4','SibSp5','SibSp8',\n",
    "                       'Parch0','Parch1','Parch2','Parch3','Parch4','Parch5','Parch6',\n",
    "                       'Fare20','Fare40','Fare60','Fare80','Fare100','Fare200','Fare300','Fare600']]\n",
    "\n",
    "df_Titanic_test = df_test[['adult','bebe','infant','jove','nen','vell','female','male','Pclass1','Pclass2',\n",
    "                       'Pclass3','Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_F','Deck_G','Deck_X',\n",
    "                       'SibSp0','SibSp1','SibSp2','SibSp3','SibSp4','SibSp5','SibSp8',\n",
    "                       'Parch0','Parch1','Parch2','Parch3','Parch4','Parch5','Parch6',\n",
    "                       'Fare20','Fare40','Fare60','Fare80','Fare100','Fare200','Fare300','Fare600']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    1.000000\n",
      "adult      -0.073645\n",
      "bebe        0.122966\n",
      "infant      0.077441\n",
      "jove       -0.008758\n",
      "nen         0.044716\n",
      "vell       -0.040857\n",
      "female      0.543351\n",
      "male       -0.543351\n",
      "Pclass1     0.285904\n",
      "Pclass2     0.093349\n",
      "Pclass3    -0.322308\n",
      "Deck_A      0.022287\n",
      "Deck_B      0.175095\n",
      "Deck_C      0.114652\n",
      "Deck_D      0.150716\n",
      "Deck_E      0.145321\n",
      "Deck_F      0.057935\n",
      "Deck_G      0.016040\n",
      "Deck_X     -0.316912\n",
      "SibSp0     -0.115867\n",
      "SibSp1      0.173076\n",
      "SibSp2      0.029796\n",
      "SibSp3     -0.037215\n",
      "SibSp4     -0.064123\n",
      "SibSp5     -0.059292\n",
      "SibSp8     -0.070234\n",
      "Parch0     -0.147408\n",
      "Parch1      0.134174\n",
      "Parch2      0.075020\n",
      "Parch3      0.033391\n",
      "Parch4     -0.053002\n",
      "Parch5     -0.028398\n",
      "Parch6     -0.026456\n",
      "Fare20      0.162583\n",
      "Fare40     -0.255496\n",
      "Fare60      0.150716\n",
      "Fare80      0.075486\n",
      "Fare100     0.051066\n",
      "Fare200     0.099358\n",
      "Fare300     0.073642\n",
      "Fare600     0.055730\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cor=df_Titanic.corr()\n",
    "print(cor.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer test de model --> Sembla que el millor era LogisticRegression\n",
    "\n",
    "exemple: https://www.kaggle.com/gautham11/building-a-scikit-learn-classification-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hem algunes proves i sembla que els millors resultats, per ara son sense considerar els Parch ni fare\n",
    "# Ara provare sense Parch nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 28)\n",
      "(891, 26) (891,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# fem un primer test..model?\n",
    "kk = array(df_Titanic)\n",
    "print (kk.shape)\n",
    "#kkx= kk[:,1:43] # amb aixo ho tenim tot en compte\n",
    "#kkx= kk[:,1:35] # sense el Fare\n",
    "kkx= kk[:,1:27] # sense Parch\n",
    "kky= kk[:,0]\n",
    "print (kkx.shape, kky.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(kkx, kky, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.003000179926554362\n",
      "fit_time  std  2.08152148782552e-06\n",
      "score_time  mean  0.0010007222493489583\n",
      "score_time  std  4.052336624139774e-07\n",
      "test_score  mean  0.7795270006736872\n",
      "test_score  std  0.018543708432345322\n",
      "train_score  mean  0.8272485009993339\n",
      "train_score  std  0.0015743539651204817\n",
      "---------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.016997416814168293\n",
      "fit_time  std  0.003558209880413971\n",
      "score_time  mean  0.007335742314656575\n",
      "score_time  std  0.003297851166324981\n",
      "test_score  mean  0.7809689276554503\n",
      "test_score  std  0.03526800478208268\n",
      "train_score  mean  0.7830216892442076\n",
      "train_score  std  0.016386606809151823\n",
      "---------------------------------\n",
      "GaussianNB(priors=None)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.0043328603108723955\n",
      "fit_time  std  0.0016993013269448278\n",
      "score_time  mean  0.0016663869222005208\n",
      "score_time  std  0.0012465771185628239\n",
      "test_score  mean  0.5603304612984434\n",
      "test_score  std  0.12966094663726946\n",
      "train_score  mean  0.5920201347249981\n",
      "train_score  std  0.1441638496868732\n",
      "---------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.00233308474222819\n",
      "fit_time  std  0.0012475960204325794\n",
      "score_time  mean  0.007999181747436523\n",
      "score_time  std  0.001633167023206261\n",
      "test_score  mean  0.8033778912408845\n",
      "test_score  std  0.010936865207221447\n",
      "train_score  mean  0.8328773410319047\n",
      "train_score  std  0.014493486456363487\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.0019988218943277993\n",
      "fit_time  std  0.00081526958781782\n",
      "score_time  mean  0.0006672541300455729\n",
      "score_time  std  0.00047182092410336313\n",
      "test_score  mean  0.8005767707927053\n",
      "test_score  std  0.010212842298446455\n",
      "train_score  mean  0.8637663779702421\n",
      "train_score  std  0.0025014583080536627\n",
      "---------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.021329720815022785\n",
      "fit_time  std  0.004027758480415301\n",
      "score_time  mean  0.002011458079020182\n",
      "score_time  std  1.556897459350947e-05\n",
      "test_score  mean  0.7963691805836258\n",
      "test_score  std  0.011707435263719194\n",
      "train_score  mean  0.8609593604263823\n",
      "train_score  std  0.004418430066657931\n",
      "---------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.11398816108703613\n",
      "fit_time  std  0.005885906146414705\n",
      "score_time  mean  0.0013330777486165364\n",
      "score_time  std  0.0004713141679983425\n",
      "test_score  mean  0.8005708612558947\n",
      "test_score  std  0.009746942151202363\n",
      "train_score  mean  0.860253164556962\n",
      "train_score  std  0.0009310603702956272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clfs = []\n",
    "clfs.append(LogisticRegression())\n",
    "clfs.append(SVC())\n",
    "clfs.append(GaussianNB())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "\n",
    "for classifier in clfs:\n",
    "    pipeline.set_params(clf = classifier)\n",
    "    scores = cross_validate(pipeline, X_train, y_train)\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('-----------------------------------')\n",
    "    for key, values in scores.items():\n",
    "            print(key,' mean ', values.mean())\n",
    "            print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultats:\n",
    "Sembla que LogisticRegression > GradientBoostingClassifier > SVC > DecisionTreeClassifier > KNeighborsClassifier > Gaussian\n",
    "\n",
    "La idea ara seria fer un Pipeline i un GridSearch i veure si trobem parametres que ens milloren model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clf',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "pipeline.set_params(clf= LogisticRegression())\n",
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'clf__penalty': ['l2'], 'clf__dual': [True], 'clf__solver': ['liblinear'], 'clf__tol': [0.0001, 0.001, 0.01], 'clf__C': array([0.1    , 0.17368, 0.24737, 0.32105, 0.39474, 0.46842, 0.54211,\n",
       "       0.61579, 0.68947, 0.76316, 0.83684, 0.91053, 0.98421, 1.05789,\n",
       "       1.13158, 1.20526, 1.... 1.27895, 1.35263, 1.42632, 1.5    ]), 'clf__fit_intercept': [True, False], 'clf__max_iter': [200]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_grid = GridSearchCV(pipeline, param_grid = [\n",
    "    { \n",
    "          \"clf__penalty\": [\"l2\"],\n",
    "          \"clf__dual\":[True],\n",
    "          \"clf__solver\":['liblinear'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":np.linspace(0.1,1.5,20),\n",
    "          \"clf__fit_intercept\":[True, False],\n",
    "          \"clf__max_iter\":[200]\n",
    "          \n",
    "    },\n",
    "    { \n",
    "          \"clf__penalty\": [\"l2\"],\n",
    "          \"clf__dual\":[False],\n",
    "          \"clf__solver\":['lbfgs','newton-cg','sag'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":np.linspace(0.1,1.5,20),\n",
    "          \"clf__fit_intercept\":[True, False],\n",
    "          \"clf__max_iter\":[200]\n",
    "         \n",
    "    },\n",
    "    { \n",
    "          \"clf__penalty\": [\"l1\"],\n",
    "          \"clf__dual\":[False],\n",
    "          \"clf__solver\":['liblinear', 'saga'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":np.linspace(0.1,1.5,20),\n",
    "          \"clf__fit_intercept\":[True, False],\n",
    "          \"clf__max_iter\":[200]\n",
    "         \n",
    "    }\n",
    "])\n",
    "\n",
    "cv_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 0.6894736842105262, 'clf__dual': True, 'clf__fit_intercept': True, 'clf__max_iter': 200, 'clf__penalty': 'l2', 'clf__solver': 'liblinear', 'clf__tol': 0.0001}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', LogisticRegression(C=0.6894736842105262, class_weight=None, dual=True,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=200,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "0.8174157303370787\n"
     ]
    }
   ],
   "source": [
    "print (cv_grid.best_params_)\n",
    "print (cv_grid.best_estimator_)\n",
    "print (cv_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "pipeline.set_params(clf= SVC())\n",
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'clf__kernel': ['linear', 'rbf'], 'clf__C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_grid = GridSearchCV(pipeline, param_grid = {\n",
    "    'clf__kernel' : ['linear', 'rbf'],\n",
    "    'clf__C' : np.linspace(0.1,1.2,12)\n",
    "})\n",
    "\n",
    "cv_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 0.4, 'clf__kernel': 'linear'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('clf', SVC(C=0.4, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7991573033707865"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 43)\n",
      "(891, 42) (891,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# fem un primer test..model?\n",
    "kk = array(df_Titanic)\n",
    "print (kk.shape)\n",
    "kkx= kk[:,1:43] # amb aixo ho tenim tot en compte\n",
    "#kkx= kk[:,1:35] # sense el Fare\n",
    "kky= kk[:,0]\n",
    "print (kkx.shape, kky.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(kkx, kky, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, LinearSVR, SVC\n",
    "clf=LinearSVC(C=1, dual=False, fit_intercept=True, intercept_scaling=10,penalty='l2',)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "clf.predict(X_test)\n",
    "\n",
    "print (\"score = %3.2f\" %(clf.score(X_test,y_test)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "clf.predict(X_test)\n",
    "\n",
    "print (\"score = %3.2f\" %(clf.score(X_test,y_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.79\n",
      "{'clf__C': 10, 'clf__dual': True, 'clf__fit_intercept': True, 'clf__penalty': 'l2', 'clf__solver': 'liblinear', 'clf__tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pipeline = Pipeline([ ('clf', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(kkx, kky, test_size=0.10)\n",
    "\n",
    "param_grid = [\n",
    "    { \n",
    "          \"clf__penalty\": [\"l2\"],\n",
    "          \"clf__dual\":[True],\n",
    "          \"clf__solver\":['liblinear'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":[0.1,1,10],\n",
    "          \"clf__fit_intercept\":[True, False]\n",
    "      },\n",
    "    { \n",
    "          \"clf__penalty\": [\"l2\"],\n",
    "          \"clf__dual\":[False],\n",
    "          \"clf__solver\":['lbfgs','newton-cg','sag'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":[0.1,1,10],\n",
    "          \"clf__fit_intercept\":[True, False]\n",
    "    },\n",
    "    { \n",
    "          \"clf__penalty\": [\"l1\"],\n",
    "          \"clf__dual\":[False],\n",
    "          \"clf__solver\":['liblinear', 'saga'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":[0.1,1,10],\n",
    "          \"clf__fit_intercept\":[True, False]\n",
    "    }\n",
    "]\n",
    "grid = GridSearchCV(pipeline, cv=6, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print (\"score = %3.2f\" %(grid.score(X_test,y_test)) )\n",
    "print (grid.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.80\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression(C=10,dual=True,fit_intercept=True, penalty='l2',solver='liblinear',tol=0.0001)\n",
    "clf=LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "clf.predict(X_test)\n",
    "\n",
    "print (\"score = %3.2f\" %(clf.score(X_test,y_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_train: mean =  0.8231320224719102  max =  0.8426966292134831  min =  0.8033707865168539\n",
      "pred_test: mean =  0.7959776536312849  max =  0.8659217877094972  min =  0.7318435754189944\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# Split train and test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(Xvec, yvec, test_size=0.20)\n",
    "pred_train = [] \n",
    "pred_test = [] \n",
    "\n",
    "for i in range(100): # entre 10 y 50...aleatorio total.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(kkx, kky, test_size=0.20)\n",
    "\n",
    "\n",
    "# My Model\n",
    "    clf = LogisticRegression()\n",
    "    #clf=LinearSVC(C=1, dual=False, fit_intercept=True, intercept_scaling=10,penalty='l2',)   \n",
    "    #clf=RandomForestClassifier(n_estimators=20)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    prediction_train = clf.predict(X_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "\n",
    "    pred_train.append(np.mean([prediction_train == y_train]))\n",
    "    pred_test.append(np.mean([prediction == y_test])) \n",
    "\n",
    "\n",
    "plt.plot(pred_train)\n",
    "plt.plot(pred_test,'.')\n",
    "plt.legend(('train','test'))\n",
    "plt.title('Logistic')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pred_test)\n",
    "print('pred_train: mean = ', np.mean(pred_train), ' max = ', np.max(pred_train),  ' min = ', np.min(pred_train), )\n",
    "print('pred_test: mean = ', np.mean(pred_test), ' max = ', np.max(pred_test), ' min = ', np.min(pred_test), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=RandomForestClassifier(n_estimators=20)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "clf.predict(X_test)\n",
    "\n",
    "print (\"score = %3.2f\" %(clf.score(X_test,y_test)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf=GaussianNB()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "clf.predict(X_test)\n",
    "\n",
    "print (\"score = %3.2f\" %(clf.score(X_test,y_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneibhours Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, LinearSVR, SVC\n",
    "#clf=SVC(kernel='rbf', degree=6, C=10) --> 0.8\n",
    "#clf=SVC(kernel='linear', degree=6, C=10) --> 0.8\n",
    "#clf=SVC(kernel='poly', degree=6, C=10, gamma='auto') --> 0.59\n",
    "clf=SVC(kernel='sigmoid', degree=6, C=10, gamma='auto') #--> 0.8\n",
    "#clf=SVC(kernel='linear', degree=6, C=10, coef0=4)\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "clf.predict(X_test)\n",
    "\n",
    "print (\"score = %3.2f\" %(clf.score(X_test,y_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
